var documenterSearchIndex = {"docs":
[{"location":"models/linear_models/#Linear-Models","page":"Linear Models","title":"Linear Models","text":"","category":"section"},{"location":"user_guide/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"user_guide/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"You can install NovaML.jl using Julia's package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"pkg> add NovaML","category":"page"},{"location":"user_guide/getting_started/#Usage","page":"Getting Started","title":"Usage","text":"","category":"section"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"The most prominent feature of NovaML is using functors (callable objects) to keep parameters as well as training and prediction. Assume model represents a supervised algorithm. The struct model keeps learned parameters and hyperparameters. It also behave as a function. ","category":"page"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"model(X, y) trains the model. \nmodel(Xnew) calculates the predictions for Xnew. ","category":"page"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"Here's a quick example of how to use NovaML.jl for a binary classification task:","category":"page"},{"location":"user_guide/getting_started/","page":"Getting Started","title":"Getting Started","text":"# Import the Iris dataset from NovaML's Datasets module\nusing NovaML.Datasets\nX, y = load_iris(return_X_y=true)\n\n# Split the data into training and test sets\n# 80% for training, 20% for testing\nusing NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.2)\n\n# Import the StandardScaler for feature scaling\nusing NovaML.PreProcessing\nscaler = StandardScaler()\nscaler.fitted # false - the scaler is not yet fitted to any data\n\n# Fit the scaler to the training data and transform it\n# NovaML uses a functor approach, so calling scaler(Xtrn) both fits and transforms\nXtrnstd = scaler(Xtrn) \n\n# Transform the test data using the fitted scaler\nXtststd = scaler(Xtst)\n\n# Import LogisticRegression from LinearModel module\nusing NovaML.LinearModel\n\n# Create a LogisticRegression model with learning rate 0.1 and 100 iterations\nlr = LogisticRegression(η=0.1, num_iter=100)\n\n# Import OneVsRestClassifier for multi-class classification\nusing NovaML.MultiClass\n# Wrap the LogisticRegression model in a OneVsRestClassifier for multi-class support\novr = OneVsRestClassifier(lr)\n\n# Fit the OneVsRestClassifier model to the standardized training data\n# NovaML uses functors, so ovr(Xtrnstd, ytrn) fits the model\novr(Xtrnstd, ytrn)\n\n# Make predictions on training and test data\n# Calling ovr(X) makes predictions using the fitted model\nŷtrn = ovr(Xtrnstd)\nŷtst = ovr(Xtststd)\n\n# Import accuracy_score metric for model evaluation\nusing NovaML.Metrics\n\n# Calculate accuracy for training and test sets\nacc_trn = accuracy_score(ytrn, ŷtrn);\nacc_tst = accuracy_score(ytst, ŷtst);\n\n# Print the results\nprintln(\"Training accuracy: $acc_trn\")\nprintln(\"Test accuracy: $acc_tst\")\n# Output:\n# Training accuracy: 0.9833333333333333\n# Test accuracy: 0.9666666666666667","category":"page"},{"location":"models/svm/#Support-Vector-Machines","page":"Support Vector Machines","title":"Support Vector Machines","text":"","category":"section"},{"location":"user_guide/model_evaluation/#Model-Evaluation","page":"Model Evaluation","title":"Model Evaluation","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"NovaML provides a range of tools and metrics for evaluating the performance of machine learning models. This page covers the main evaluation techniques and metrics available in NovaML.","category":"page"},{"location":"user_guide/model_evaluation/#Classification-Metrics","page":"Model Evaluation","title":"Classification Metrics","text":"","category":"section"},{"location":"user_guide/model_evaluation/#Accuracy-Score","page":"Model Evaluation","title":"Accuracy Score","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Accuracy is the ratio of correct predictions to total predictions.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Metrics\n\ny = [0, 1, 1, 0, 1];\nŷ = [0, 1, 0, 0, 1]\n\naccuracy = accuracy_score(y, ŷ);\nprintln(\"Accuracy: $accuracy\")","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Following example load the Wisconsin Breast Cancer dataset, splits it to training and test sets and traing a logistic regression model with the training set. Then it calculates the accuracy_score for training and test sets. ","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Datasets: load_breast_cancer\nX, y = load_breast_cancer(return_X_y=true)\n\nusing NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.2,\n                                          stratify=y, random_state=1)\n\nusing NovaML.LinearModel\nlr = LogisticRegression()\n\n# train the model\nlr(Xtrn, ytrn)\n\nusing NovaML.Metrics\nŷtrn, ŷtst = lr(Xtrn), lr(Xtst)\n\n# training data accuracy\naccuracy_score(ŷtrn, ytrn)\n# test data accuracy\naccuracy_score(ŷtst, ytst) ","category":"page"},{"location":"user_guide/model_evaluation/#Confusion-Matrix","page":"Model Evaluation","title":"Confusion Matrix","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"The confusion matrix provides a detailed breakdown of correct and incorrect classifications for each class. You can use confusion_matrix to create the confusion matrix and display_confusion_matrix to display it with labels. ","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Metrics\nconfmat = confusion_matrix(ytst, ŷtst)\n# 2×2 Matrix{Int64}:\n#  71   1\n#   5  37","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"display_confusion_matrix(confmat)\n#        1    2\n#    ----------\n# 1 | 71.0  1.0\n# 2 |  5.0 37.0","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"We can create a better looking confusion matrix plot using the following function:","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using Plots\nusing Plots.PlotMeasures\n\nfunction plot_confusion_matrix(confmat::Matrix)\n    n = size(confmat, 1)\n    \n    heatmap(confmat, \n            c=:Blues, \n            alpha=0.3, \n            aspect_ratio=:equal, \n            size=(300, 300),\n            xrotation=0,\n            xticks=1:n, \n            yticks=1:n,\n            xlims=(0.5, n+0.5), \n            ylims=(0.5, n+0.5),\n            right_margin=5mm,\n            xlabel=\"Predicted label\",\n            ylabel=\"True label\",\n            xmirror=true, \n            framestyle=:box, \n            legend=nothing)\n    \n    for i in 1:n, j in 1:n\n        annotate!(j, i, text(string(confmat[i,j]), :center, 10))\n    end\n    \n    plot!(yflip=true)\n    \n    display(current())\nend","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"plot_confusion_matrix(confmat)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"(Image: Confusion matrix plot)","category":"page"},{"location":"user_guide/model_evaluation/#Precision,-Recall,-and-F1-Score","page":"Model Evaluation","title":"Precision, Recall, and F1 Score","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"These metrics provide more detailed insights into model performance, especially for imbalanced datasets.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Metrics\n\nprecision_score(ytst, ŷtst)\n# 0.9736842105263158\nrecall_score(ytst, ŷtst)\n# 0.8809523809523809\nf1_score(ytst, ŷtst)\n# 0.925","category":"page"},{"location":"user_guide/model_evaluation/#Matthews-Correlation-Coefficient","page":"Model Evaluation","title":"Matthews Correlation Coefficient","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"The Matthews Correlation Coefficient (MCC) is a balanced measure for binary classification problems. It takes into account true and false positives and negatives, providing a balanced measure even for classes of different sizes.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"MCC = fracTN times TP - FN times FPsqrt(TP+FP)(TP+FN)(TN+FP)(TN+FN)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"where","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"TP: True Positive\nFP: False Positive\nTN: True Negative\nFN: False Negative","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"MCC ranges from -1 to +1:","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"+1 represents a perfect prediction\n0 represents no better than random prediction\n1 indicates total disagreement between prediction and observation","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Metrics\nmatthews_corrcoef(ytst, ŷ)\n#0.8872442622820285","category":"page"},{"location":"user_guide/model_evaluation/#ROC-Curve-and-AUC","page":"Model Evaluation","title":"ROC Curve and AUC","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"For binary classification problems, you can compute the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC).","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"ŷprobs = lr(Xtst, type=:probs)[:, 2]\nfpr, tpr, _ = roc_curve(ytst, ŷprobs);\n# auc score\nroc_auc = auc(fpr, tpr)\n# 0.9923941798941799","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"We can also plot the receiver operating characteristic curve using fpr and tpr values.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"plot(fpr, tpr, color=:blue, linestyle=:solid, \n     label=\"Logistic Regression (auc = $(round(roc_auc, digits=2)))\",\n     xlabel=\"False Positive\",\n     ylabel=\"True Positive\",\n     title=\"Receiver Operating Characteristic Curve\")     \nplot!([0, 1], [0, 1], color=:gray, linestyle=:dash, linewidth=2, label=\"Random\")","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"(Image: ROC Curve)","category":"page"},{"location":"user_guide/model_evaluation/#Regression-Metrics","page":"Model Evaluation","title":"Regression Metrics","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"We will use the Boston Housing Data in the section. ","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"First let's import and prepare the data. ","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Datasets\nX, y = load_boston(return_X_y=true)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Prepare the training and test data sets.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.3, random_state=123)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Create and fit the linear regression model.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"lr = LinearRegression()\nlr(Xtrn, ytrn)\nŷtrn = lr(Xtrn)\nŷtst = lr(Xtst)","category":"page"},{"location":"user_guide/model_evaluation/#Mean-Absolute-Error-(MAE)","page":"Model Evaluation","title":"Mean Absolute Error (MAE)","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"MAE measures the average magnitude of errors in a set of predictions, without considering their direction.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Metrics\nmaetrn = mae(ytrn, ŷtrn)\nmaetst = mae(ytst, ŷtst)\nmaetrn, martst\n#(0.07, 0.09)","category":"page"},{"location":"user_guide/model_evaluation/#Mean-Squared-Error-(MSE)","page":"Model Evaluation","title":"Mean Squared Error (MSE)","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"MSE measures the average squared difference between the estimated values and the actual value.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"msetrn = mse(ytrn, ŷtrn)\nmsetst = mse(ytst, ŷtst)\nmsetrn, msetst\n#(0.0093, 0.0128)","category":"page"},{"location":"user_guide/model_evaluation/#R-squared-Score","page":"Model Evaluation","title":"R-squared Score","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"R-squared (R²) provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"r2_score(ytrn, ŷtrn)\n# 0.9992871136620213\nr2_score(ytst, ŷtst)\n# 0.9991476060812577","category":"page"},{"location":"user_guide/model_evaluation/#Clustering-Metrics","page":"Model Evaluation","title":"Clustering Metrics","text":"","category":"section"},{"location":"user_guide/model_evaluation/#Silhouette-Score","page":"Model Evaluation","title":"Silhouette Score","text":"","category":"section"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"The Silhouette Score is used to evaluate the quality of clusters in clustering algorithms.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Let's first create an artificial dataset.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Datasets\nX, y = make_blobs(\n    n_samples=150,\n    n_features=2,\n    centers=3,\n    cluster_std=0.5,\n    shuffle=true,\n    random_state=123)\n\nusing Plots\nbegin\n    scatter(X[:, 1], X[:, 2],\n            color=:white,\n            markerstrokecolor=:black,\n            markersize=6,\n            xlabel=\"Feature 1\",\n            ylabel=\"Feature 2\",\n            grid=true,\n            legend=false)\n    \n    plot!(size=(600, 400), margin=5Plots.mm)\nend","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"(Image: Cluster Data)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Next, create the clustering algorithm.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using NovaML.Cluster\nkm = KMeans(\n    n_clusters=3,\n    init=\"k-means++\",\n    n_init=10,\n    max_iter=300,\n    tol=1e-04,\n    random_state=0)\n\nkm(X)","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"Now we can create the Silhouette Plot using the silhouette_samples from NovaML.Metrics.","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"using Statistics\nusing ColorSchemes\nusing NovaML.Metrics\n\nfunction plot_silhoutte(km::KMeans)\n    ykm = km.labels_\n    cluster_labels = sort(unique(km.labels_))\n    n_clusters = length(cluster_labels)\n    silhouette_vals = silhouette_samples(X, km.labels_, metric=\"euclidean\")\n    δ = 1. / (length(silhouette_vals)+20)\n    yval = 10δ\n    \n    p = plot(xlabel=\"Silhouette coefficient\", label=\"Cluster\", title=\"Silhouette Plot\", legend=false, ylims=(0.0, 1.0), xlims=(0.0, 1.0), ylabel=\"Cluster\");\n    for (i, c) in enumerate(cluster_labels)\n        c_silhouette_vals = silhouette_vals[ykm.==c]\n        sort!(c_silhouette_vals)\n        color = get(ColorSchemes.jet, i/n_clusters)\n        for xval in c_silhouette_vals\n            plot!(p, [0, xval], [yval, yval], color=color)\n            yval += δ\n        end\n    end\n    silhouette_avg = mean(silhouette_vals)\n    vline!([silhouette_avg], color=:red, linestyle=:dash, lw=2)\n    \n    start = (1-20δ)/6\n    stop = (1 - 10δ) - (1-20δ)/6\n    \n    yticks!(p, range(start, stop, length=n_clusters), string.(cluster_labels))    \nend","category":"page"},{"location":"user_guide/model_evaluation/","page":"Model Evaluation","title":"Model Evaluation","text":"(Image: silhouette plot)","category":"page"},{"location":"user_guide/feature_engineering/#Feature-Engineering","page":"Feature Engineering","title":"Feature Engineering","text":"","category":"section"},{"location":"user_guide/core_concepts/#Core-Concepts","page":"Core Concepts","title":"Core Concepts","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"NovaML.jl is designed with simplicity, flexibility, and performance in mind. Understanding the core concepts will help you make the most of the library.","category":"page"},{"location":"user_guide/core_concepts/#Functor-based-API","page":"Core Concepts","title":"Functor-based API","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"One of the distinguishing features of NovaML is its use of functors (callable objects) for model training, prediction, and data transformation. This approach leverages Julia's multiple dispatch system to provide a clean and intuitive API.","category":"page"},{"location":"user_guide/core_concepts/#Models","page":"Core Concepts","title":"Models","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"For supervised learning models:","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"model(X, y): Trains the model on input data X and target values y.\nmodel(X): Makes predictions on new data X.\nmodel(X, type=:probs): Computes probability predictions (for classifiers).","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"For unsupervised learning models:","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"model(X): Fits the model to the data X.","category":"page"},{"location":"user_guide/core_concepts/#Transformers","page":"Core Concepts","title":"Transformers","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"For data preprocessing and feature engineering:","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"transformer(X): Fits the transformer to the data X and applies the transformation.\ntransformer(X, type=:inverse_transform): Applies the inverse transformation (if available).","category":"page"},{"location":"user_guide/core_concepts/#Abstract-Types","page":"Core Concepts","title":"Abstract Types","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"NovaML uses a hierarchy of abstract types to organize its components:","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"AbstractModel: Base type for all machine learning models.\nAbstractMultiClass: Subtype of AbstractModel for multi-class classifiers.\nAbstractScaler: Base type for scaling transformers.","category":"page"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"These abstract types allow for easy extension and customization of the library.","category":"page"},{"location":"user_guide/core_concepts/#Unified-API","page":"Core Concepts","title":"Unified API","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"NovaML strives to provide a consistent interface across different types of models and tasks. This unified API makes it easier to switch between different algorithms and encourages experimentation.","category":"page"},{"location":"user_guide/core_concepts/#Pipelines","page":"Core Concepts","title":"Pipelines","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"NovaML supports the creation of machine learning pipelines, which allow you to chain multiple steps of data preprocessing and model training into a single object. Pipelines can be treated as models themselves, simplifying complex workflows.","category":"page"},{"location":"user_guide/core_concepts/#Hyperparameter-Tuning","page":"Core Concepts","title":"Hyperparameter Tuning","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"The library includes tools for automated hyperparameter tuning, such as grid search and random search. These can be easily integrated with cross-validation techniques to find optimal model configurations.","category":"page"},{"location":"user_guide/core_concepts/#Metrics-and-Evaluation","page":"Core Concepts","title":"Metrics and Evaluation","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"NovaML provides a range of metrics for evaluating model performance, as well as utilities for cross-validation and model selection.","category":"page"},{"location":"user_guide/core_concepts/#Data-Handling","page":"Core Concepts","title":"Data Handling","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"The library is designed to work seamlessly with Julia's native array types and supports both dense and sparse data structures.","category":"page"},{"location":"user_guide/core_concepts/#Modules-and-Methods","page":"Core Concepts","title":"Modules and Methods","text":"","category":"section"},{"location":"user_guide/core_concepts/#Datasets","page":"Core Concepts","title":"Datasets","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"load_boston: Loads the Boston Housing dataset, a classic regression problem. It contains information about housing in the Boston area, with 13 features and a target variable representing median home values.\nload_iris: Provides access to the famous Iris flower dataset, useful for classification tasks. It includes 150 samples with 4 features each, categorized into 3 different species of Iris.\nload_breast_cancer: Loads the Wisconsin Breast Cancer dataset, a binary classification problem. It contains features computed from digitized images of breast mass, with the goal of predicting whether a tumor is malignant or benign.\nload_wine: Offers the Wine recognition dataset, suitable for multi-class classification. It includes 13 features derived from chemical analysis of wines from three different cultivars in Italy.\nmake_blobs: Generates isotropic Gaussian blobs for clustering or classification tasks. This function allows you to create synthetic datasets with a specified number of samples, features, and centers, useful for testing and benchmarking algorithms.\nmake_moons: Generates a 2D binary classification dataset in the shape of two interleaving half moons. This synthetic dataset is ideal for visualizing and testing classification algorithms, especially those that can handle non-linear decision boundaries.","category":"page"},{"location":"user_guide/core_concepts/#PreProcessing","page":"Core Concepts","title":"PreProcessing","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"StandardScaler: Standardize features by removing the mean and scaling to unit variance\nMinMaxScaler: Scale features to a given range\nLabelEncoder: Encode categorical features as integers\nOneHotEncoder: Encode categorical features as one-hot vectors\nPolynomialFeatures: Generate polynomial and interaction features up to a specified degree","category":"page"},{"location":"user_guide/core_concepts/#Impute","page":"Core Concepts","title":"Impute","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"SimpleImputer: A basic imputation transformer for filling in missing values in datasets using strategies such as mean, median, most frequent, or constant value.","category":"page"},{"location":"user_guide/core_concepts/#FeatureExtraction","page":"Core Concepts","title":"FeatureExtraction","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"CountVectorizer: Convert a collection of text documents to a matrix of token counts, useful for text feature extraction\nTfidfVectorizer: Transform a collection of raw documents to a matrix of TF-IDF features, combining the functionality of CountVectorizer with TF-IDF weighting ","category":"page"},{"location":"user_guide/core_concepts/#Decomposition","page":"Core Concepts","title":"Decomposition","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"LatentDirichletAllocation: A generative statistical model that allows sets of observations to be explained by unobserved groups. It's commonly used for topic modeling in natural language processing.\nPCA: Principal Component Analysis, a dimensionality reduction technique that identifies the axes of maximum variance in high-dimensional data and projects it onto a lower-dimensional subspace.","category":"page"},{"location":"user_guide/core_concepts/#LinearModels","page":"Core Concepts","title":"LinearModels","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"Adaline: Adaptive Linear Neuron\nElasticNet: Linear regression with combined L1 and L2 priors as regularizer, balancing between Lasso and Ridge models\nLasso: Linear Model trained with L1 prior as regularizer, useful for producing sparse models\nLinearRegression: Linear regression algorithm\nLogisticRegression: Binary and multiclass logistic regression\nPerceptron: Simple perceptron algorithm\nRANSACRegression: Robust regression using Random Sample Consensus (RANSAC) algorithm. It's particularly effective for fitting models in the presence of significant outliers in the data.\nRidge: Linear regression with L2 regularization, useful for dealing with multicollinearity in data","category":"page"},{"location":"user_guide/core_concepts/#MultiClass","page":"Core Concepts","title":"MultiClass","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"MulticlassPerceptron: An extension of the binary perceptron algorithm for multi-class classification problems. It learns a linear decision boundary for each class and updates weights based on misclassifications.\nOneVsRestClassifier: A strategy for multi-class classification that fits one binary classifier per class, treating the class as positive and all others as negative. It's versatile and can be used with any base binary classifier.","category":"page"},{"location":"user_guide/core_concepts/#Neighbors","page":"Core Concepts","title":"Neighbors","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"KNeighborsClassifier: K-nearest neighbors classifier","category":"page"},{"location":"user_guide/core_concepts/#SVM","page":"Core Concepts","title":"SVM","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"SVC: Support Vector Classifier. Binary classification which supports linear and RBF kernels. Doesn't support multiclass classification yet. ","category":"page"},{"location":"user_guide/core_concepts/#Tree","page":"Core Concepts","title":"Tree","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"DecisionTreeClassifier: Decision tree for classification\nDecisionTreeRegressor: Decision tree for regression","category":"page"},{"location":"user_guide/core_concepts/#Ensemble","page":"Core Concepts","title":"Ensemble","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"AdaBoostClassifier: An ensemble method that sequentially applies a base classifier to reweighted versions of the training data, giving more emphasis to incorrectly classified instances in subsequent iterations\nBaggingClassifier: A meta-estimator that fits base classifiers on random subsets of the original dataset and aggregates their predictions to form a final prediction.\nGradientBoostingClassifier: An ensemble method that builds an additive model in a forward stage-wise fashion, allowing for the optimization of arbitrary differentiable loss functions. It uses decision trees as base learners and combines them to create a strong predictive model.\nRandomForestClassifier: An ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes of the individual trees.\nRandomForestRegressor: An ensemble method that builds multiple decision trees for regression tasks and predicts by averaging their outputs. It combines bagging with random feature selection to create a robust, accurate model that often resists overfitting.\nVotingClassifier: A classifier that combines multiple machine learning classifiers and uses a majority vote or the average predicted probabilities to predict the class labels.","category":"page"},{"location":"user_guide/core_concepts/#Cluster","page":"Core Concepts","title":"Cluster","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"AgglomerativeClustering: A hierarchical clustering algorithm that builds nested clusters by merging or splitting them successively. This bottom-up approach is versatile and can create clusters of various shapes.\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise, a density-based clustering algorithm that groups together points that are closely packed together, marking points that lie alone in low-density regions as outliers.\nKMeans: A popular and simple clustering algorithm that partitions n observations into k clusters, where each observation belongs to the cluster with the nearest mean (cluster centroid). It's efficient for large datasets but assumes spherical clusters of similar size.","category":"page"},{"location":"user_guide/core_concepts/#Metrics","page":"Core Concepts","title":"Metrics","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"accuracy_score: Calculates the accuracy classification score, i.e., the proportion of correct predictions.\nauc: Computes the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve, evaluating the overall performance of a binary classifier.\nconfusion_matrix: Computes a confusion matrix to evaluate the accuracy of a classification. It shows the counts of true positive, false positive, true negative, and false negative predictions.\nmean_absolute_error, mae: Computes the average absolute difference between estimated and true values. This metric is robust to outliers and provides a linear measure of error. mae is an alias for mean_absolute_error.\nmean_squared_error, mse: Computes the average squared difference between estimated and true values. mse is an alias for mean_squared_error.\nr2_score: Calculates the coefficient of determination (R²), measuring how well future samples are likely to be predicted by the model.\nadj_r2_score: Computes the adjusted R² score, which accounts for the number of predictors in the model, penalizing unnecessary complexity.\nf1_score: Computes the F1 score, which is the harmonic mean of precision and recall, providing a balance between the two.\nmatthews_corcoef: Calculates the Matthews correlation coefficient (MCC), a measure of the quality of binary classifications, considering all four confusion matrix categories.\nprecision_score: Computes the precision score, which is the ratio of true positive predictions to the total predicted positives.\nrecall_score: Computes the recall score, which is the ratio of true positive predictions to the total actual positives.\nroc_auc_score: Computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC), providing an aggregate measure of classifier performance.\nroc_curve: Produces the values (fpr, tpr) to plot the Receiver Operating Characteristic (ROC) curve, showing the trade-off between true positive rate and false positive rate at various threshold settings.\nsilhouette_samples: Computes the silhouette coefficient for each sample in a dataset, measuring how similar an object is to its own cluster compared to other clusters. This metric is useful for evaluating the quality of clustering algorithms.","category":"page"},{"location":"user_guide/core_concepts/#ModelSelection","page":"Core Concepts","title":"ModelSelection","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"cross_val_score: Apply cross validation score\nGridSearchCV: Perform exhaustive search over specified parameter values for an estimator.\nlearning_curve: Generate learning curves to evaluate model performance as a function of the number of training samples, helping to diagnose bias and variance problems\nRandomSearchCV: Perform randomized search over specified parameter distributions for an estimator. RandomSearchCV is often more efficient than GridSearchCV for hyperparameter optimization, especially when the parameter space is large or when some parameters are more important than others.\nStratifiedKFold: Provides stratified k-fold cross-validator, ensuring that the proportion of samples for each class is roughly the same in each fold\ntrain_test_split: Split arrays or matrices into random train and test subsets\nvalidation_curve: Determine training and validation scores for varying parameter values, helping to assess how a model's performance changes with respect to a specific hyperparameter and aiding in hyperparameter tuning","category":"page"},{"location":"user_guide/core_concepts/#Pipelines-2","page":"Core Concepts","title":"Pipelines","text":"","category":"section"},{"location":"user_guide/core_concepts/","page":"Core Concepts","title":"Core Concepts","text":"pipe: NovaML supports piped data transformation and model training via |> operator or NovaML.Pipelines.pipe ","category":"page"},{"location":"user_guide/preprocessing/#Data-Preprocessing","page":"Data Preprocessing","title":"Data Preprocessing","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Data preprocessing is a crucial step in any machine learning pipeline. NovaML.jl provides a range of tools for cleaning, transforming, and preparing your data for model training. This page covers the main preprocessing techniques available in NovaML.","category":"page"},{"location":"user_guide/preprocessing/#Scaling","page":"Data Preprocessing","title":"Scaling","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Scaling features is often necessary to ensure that all features contribute equally to the model training process. NovaML offers several scaling methods:","category":"page"},{"location":"user_guide/preprocessing/#StandardScaler","page":"Data Preprocessing","title":"StandardScaler","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Standardizes features by removing the mean and scaling to unit variance.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.Datasets\niris = load_iris()\nX = iris[\"data\"][:, 3:4]\ny = iris[\"target\"]","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Split data to train and test sets.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.3, stratify=y)","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Fit and transform StandardScaler.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.PreProcessing\n\nstdscaler = StandardScaler()\n\n# fit and transform\nXtrnstd = stdscaler(Xtrn)\n\n# transform\nXtststd = stdscaler(Xtst)\n\n# inverse transform\nXtrn = stdscaler(Xtrnstd, type=:inverse)","category":"page"},{"location":"user_guide/preprocessing/#MinMaxScaler","page":"Data Preprocessing","title":"MinMaxScaler","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Scales features to a number between [0, 1].","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"minmax = MinMaxScaler()\n\n# fit & transform\nXtrn_mm = minmax(Xtrn)\n# transform\nXtst_mm = minmax(Xtst)\n# inverse transform\nXtrn = minmax(Xtrn_mm, type=:inverse)","category":"page"},{"location":"user_guide/preprocessing/#Encoding","page":"Data Preprocessing","title":"Encoding","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Categorical variables often need to be encoded into numerical form for machine learning algorithms.","category":"page"},{"location":"user_guide/preprocessing/#LabelEncoder","page":"Data Preprocessing","title":"LabelEncoder","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Encodes target labels with value between 0 and n_classes-1.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.PreProcessing\n\nlblencode = LabelEncoder()\n\nlabels = [\"M\", \"L\", \"XL\", \"M\", \"L\", \"M\"]\n\n# Label encode labels\nlabels = lblencode(labels)\n\n# Get the labels back\nlblencode(labels, :inverse)","category":"page"},{"location":"user_guide/preprocessing/#OneHotEncoder","page":"Data Preprocessing","title":"OneHotEncoder","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Encodes categorical features as a one-hot numeric array.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.PreProcessing\n\nlabels = [\"M\", \"L\", \"XL\", \"M\", \"L\", \"M\"]\n\nohe = OneHotEncoder()\nonehot = ohe(labels)\nohe(onehot, :inverse)","category":"page"},{"location":"user_guide/preprocessing/#PolynomialFeatures","page":"Data Preprocessing","title":"PolynomialFeatures","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Generates polynomial and interaction features.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.PreProcessing\n\nX = rand(5, 2)\n# 5×2 Matrix{Float64}:\n#  0.85245   0.405935\n#  0.139957  0.380467\n#  0.730332  0.0418465\n#  0.051091  0.570372\n#  0.730245  0.128763\n\npoly = PolynomialFeatures(\n    degree=2,\n    interaction_only=false,\n    include_bias=true)\n\nXnew = poly(X)\n# 5×6 Matrix{Float64}:\n#  1.0  0.85245   0.405935   0.72667     0.346039   0.164783\n#  1.0  0.139957  0.380467   0.0195881   0.0532491  0.144755\n#  1.0  0.730332  0.0418465  0.533384    0.0305618  0.00175113\n#  1.0  0.051091  0.570372   0.00261029  0.0291409  0.325324\n#  1.0  0.730245  0.128763   0.533258    0.0940282  0.0165798","category":"page"},{"location":"user_guide/preprocessing/#Imputation","page":"Data Preprocessing","title":"Imputation","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Missing data is a common issue in real-world datasets. NovaML provides tools for handling missing values. The strategy argument must be one of :mean, :median, :most_frequent, or :constant.","category":"page"},{"location":"user_guide/preprocessing/#SimpleImputer","page":"Data Preprocessing","title":"SimpleImputer","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Imputes missing values using a variety of strategies.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"X = [1.0   2.0   3.0       4.0\n     5.0   6.0    missing  8.0\n    10.0  11.0  12.0        missing]\n\nusing NovaML.Impute\nimputer = SimpleImputer(strategy=:mean)\nXimp = imputer(X)\n\n# 3×4 Matrix{Union{Missing, Float64}}:\n#   1.0   2.0   3.0  4.0\n#   5.0   6.0   7.5  8.0\n#  10.0  11.0  12.0  6.0","category":"page"},{"location":"user_guide/preprocessing/#Pipelines","page":"Data Preprocessing","title":"Pipelines","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"You can combine multiple preprocessing steps into a single pipeline for easier management and application.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.PreProcessing: StandardScaler\nusing NovaML.Decomposition: PCA\nusing NovaML.LinearModel: LogisticRegression\n\nsc = StandardScaler()\npca = PCA(n_components=2)\nlr = LogisticRegression()\n\n# transform the data and fit the model \nXtrn |> sc |> pca |> X -> lr(X, ytrn)\n\n# make predictions\nŷtst = Xtst |> sc |> pca |> lr","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"It is also possible to create pipelines using NovaML's Pipe constructor:","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"using NovaML.Pipelines: pipe\n\n# create a pipeline\npipe = pipe(\n   StandardScaler(),\n   PCA(n_components=2),\n   LogisticRegression())\n\n# fit the pipe\npipe(Xtrn, ytrn)\n# make predictions\nŷ = pipe(Xtst) \n# make probability predictions\nŷprobs = pipe(Xtst, type=:probs)","category":"page"},{"location":"user_guide/preprocessing/#Text-Preprocessing","page":"Data Preprocessing","title":"Text Preprocessing","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"For text data, NovaML offers vectorization techniques:","category":"page"},{"location":"user_guide/preprocessing/#CountVectorizer","page":"Data Preprocessing","title":"CountVectorizer","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Converts a collection of text documents to a matrix of token counts.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"docs = [\n    \"Julia was designed for high performance\",\n    \"Julia uses multiple dispatch as a paradigm\",\n    \"Julia is dynamically typed, feels like a scripting language\",\n    \"But can also optionally be separately compiled\",\n    \"Julia is an open source project\"];\n\nusing NovaML.FeatureExtraction\n\ncountvec = CountVectorizer();\nbag = countvec(docs);\ncountvec.vocabulary\n\n# Dict{String, Int64} with 30 entries:\n#   \"scripting\"   => 25\n#   \"high\"        => 14\n#   \"feels\"       => 12\n#   \"is\"          => 15\n#   \"separately\"  => 26\n#   \"language\"    => 17\n#   \"typed\"       => 28\n#   \"but\"         => 6\n#   \"a\"           => 1\n#   \"for\"         => 13\n#   \"optionally\"  => 21\n#   \"paradigm\"    => 22\n#   \"was\"         => 30\n#   \"dynamically\" => 11\n#   \"also\"        => 2\n#   \"an\"          => 3\n#   \"multiple\"    => 19\n#   \"be\"          => 5\n#   \"julia\"       => 16\n#   \"project\"     => 24\n#   \"uses\"        => 29\n#   \"source\"      => 27\n#   \"open\"        => 20\n#   \"performance\" => 23\n#   \"compiled\"    => 8\n#   \"designed\"    => 9\n#   \"as\"          => 4\n#   \"can\"         => 7\n#   \"like\"        => 18\n#   \"dispatch\"    => 10\n\ncountvec(bag, type=:inverse)\n# 5-element Vector{String}:\n#  \"designed for high julia performance was\"\n#  \"a as dispatch julia multiple paradigm uses\"\n#  \"a dynamically feels is julia language like scripting typed\"      \n#  \"also be but can compiled optionally separately\"\n#  \"an is julia open project source\"","category":"page"},{"location":"user_guide/preprocessing/#TfidfVectorizer","page":"Data Preprocessing","title":"TfidfVectorizer","text":"","category":"section"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Converts a collection of raw documents to a matrix of TF-IDF features.","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"tfidf = TfidfVectorizer()\n\nresult = tfidf(docs)\ntfidf.vocabulary\ntfidf(result, type=:inverse)\n\nnew_docs = [\" The talk on the Unreasonable Effectiveness of Multiple Dispatch explains why it works so well.\"]\nXnew = tfidf(new_docs)\n","category":"page"},{"location":"user_guide/preprocessing/","page":"Data Preprocessing","title":"Data Preprocessing","text":"Most preprocessing transforms in NovaML follow the functor pattern: transform(X) both fits the transformer to the data and applies the transformation. For separate fitting and transforming (e.g., when you want to apply the same transformation to test data), you can use the fitted transformer directly on new data.","category":"page"},{"location":"user_guide/model_training/#Model-Training","page":"Model Training","title":"Model Training","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"NovaML.jl provides a unified and intuitive interface for training machine learning models. This page covers the basics of model training, including how to initialize, fit, and use different types of models.","category":"page"},{"location":"user_guide/model_training/#General-Training-Procedure","page":"Model Training","title":"General Training Procedure","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"Most supervised learning models in NovaML follow this general pattern:","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"Initialize the model with desired hyperparameters\nCall the model with training data to fit it\nUse the fitted model to make predictions on new data","category":"page"},{"location":"user_guide/model_training/#Classification-Models","page":"Model Training","title":"Classification Models","text":"","category":"section"},{"location":"user_guide/model_training/#Logistic-Regression","page":"Model Training","title":"Logistic Regression","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.LinearModel\n\n# Initialize the model\nlr = LogisticRegression(\n    η=0.1,          # η: \\eta\n    num_iter=100,\n    solver=:lbfgs,\n    λ=0.1)          # λ: \\lambda \n\n# Fit the model\nlr(Xtrain, ytrain)\n\n# Make predictions\nŷtrn = lr(Xtrain) # ŷ: y\\hat\nŷtst = lr(Xtst)\n\n# Get probability predictions\nŷprobs = lr(Xtest, type=:probs)","category":"page"},{"location":"user_guide/model_training/#Decision-Tree-Classifier","page":"Model Training","title":"Decision Tree Classifier","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.Tree\n\n# Initialize the model\ndt = DecisionTreeClassifier(max_depth=5)\n\n# Fit the model\ndt(Xtrain, ytrain)\n\n# Make predictions\nŷ = dt(Xtest)","category":"page"},{"location":"user_guide/model_training/#RandomForestClassifier","page":"Model Training","title":"RandomForestClassifier","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.Ensemble\n\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=500, max_depth=5)\n\n# Fit the model\nrf(Xtrain, ytrain)\n\n# Make predictions\nŷ = rf(Xtest)","category":"page"},{"location":"user_guide/model_training/#Regression-Models","page":"Model Training","title":"Regression Models","text":"","category":"section"},{"location":"user_guide/model_training/#Linear-Regression","page":"Model Training","title":"Linear Regression","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.LinearModel\n\n# Initialize the model\nlr = LinearRegression()\n\n# Fit the model\nlr(Xtrain, ytrain)\n\n# Make predictions\nŷ = lr(Xtest)","category":"page"},{"location":"user_guide/model_training/#Decision-Tree-Regressor","page":"Model Training","title":"Decision Tree Regressor","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.Tree\n\n# Initialize the model\ndt = DecisionTreeRegressor(max_depth=5)\n\n# Fit the model\ndt(Xtrain, ytrain)\n\n# Make predictions\nŷ = dt(Xtest)","category":"page"},{"location":"user_guide/model_training/#Clustering-Models","page":"Model Training","title":"Clustering Models","text":"","category":"section"},{"location":"user_guide/model_training/#K-Means","page":"Model Training","title":"K-Means","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.Cluster\n\n# Initialize the model\nkmeans = KMeans(n_clusters=3)\n\n# Fit the model\nkmeans(X)\n\n# Get cluster assignments\nlabels = kmeans(X)","category":"page"},{"location":"user_guide/model_training/#Support-Vector-Machines","page":"Model Training","title":"Support Vector Machines","text":"","category":"section"},{"location":"user_guide/model_training/#Support-Vector-Classifier","page":"Model Training","title":"Support Vector Classifier","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.SVM\n\n# Initialize the model\nsvm = SVC(kernel=:rbf, C=1.0)\n\n# Fit the model\nsvm(Xtrain, ytrain)\n\n# Make predictions\ny_pred = svm(Xtest)","category":"page"},{"location":"user_guide/model_training/#Multi-class-Classification","page":"Model Training","title":"Multi-class Classification","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"using NovaML.MultiClass\nusing NovaML.LinearModel\n\n# Initialize the base model\nlr = LogisticRegression(η=0.1, num_iter=100)\n\n# Wrap it in a OneVsRestClassifier\novr = OneVsRestClassifier(lr)\n\n# Fit the model\novr(Xtrain, ytrain)\n\n# Make predictions\ny_pred = ovr(Xtest)","category":"page"},{"location":"user_guide/model_training/#Model-Parameters","page":"Model Training","title":"Model Parameters","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"After training, you can access model parameters:","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"# For LinearRegression\ncoefficients = lr.w\nintercept = lr.b\n\n# For DecisionTreeClassifier\nfeature_importances = dt.feature_importances_","category":"page"},{"location":"user_guide/model_training/#Handling-Convergence-and-Iterations","page":"Model Training","title":"Handling Convergence and Iterations","text":"","category":"section"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"Some models, like LogisticRegression, allow to specify the number of iterations and learning rate:","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"lr = LogisticRegression(η=0.01, num_iter=1000)","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"You can inspect the training process by looking at the loss history:","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"losses = lr.losses","category":"page"},{"location":"user_guide/model_training/","page":"Model Training","title":"Model Training","text":"For the full list of models you can check Core Concepts page or API Reference.","category":"page"},{"location":"#NovaML.jl","page":"Home","title":"NovaML.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"⚠️ IMPORTANT NOTE: NovaML.jl is currently in alpha stage. It is under active development and may contain bugs or incomplete features. Users should exercise caution and avoid using NovaML.jl in production environments at this time. We appreciate your interest and welcome feedback and contributions to help improve the package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"NovaML.jl aims to provide a comprehensive and user-friendly machine learning framework written in Julia. Its objective is providing a unified API for various machine learning tasks, including supervised learning, unsupervised learning, and preprocessing, feature engineering etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Main objective of NovaML.jl is to increase the usage of Julia in daily data science and machine learning activities among students and practitioners.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Currently, the module and function naming in NovaML is similar to that of Scikit Learn to provide a familiarity to data science and machine learning practitioners. However, NovaML is not a wrapper of ScikitLearn.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Unified API using Julia's multiple dispatch and functor-style callable objects\nAlgorithms for classification, regression, and clustering\nPreprocessing tools for data scaling, encoding, and imputation\nModel selection and evaluation utilities\nEnsemble methods","category":"page"},{"location":"models/clustering/#Clustering-Algorithms","page":"Clustering","title":"Clustering Algorithms","text":"","category":"section"}]
}
