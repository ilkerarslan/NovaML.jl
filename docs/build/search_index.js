var documenterSearchIndex = {"docs":
[{"location":"models/clustering.html#Clustering-Algorithms","page":"Clustering","title":"Clustering Algorithms","text":"","category":"section"},{"location":"models/clustering.html#Agglomerative-Clustering","page":"Clustering","title":"Agglomerative Clustering","text":"","category":"section"},{"location":"models/clustering.html#DBSCAN","page":"Clustering","title":"DBSCAN","text":"","category":"section"},{"location":"models/clustering.html#KMeans","page":"Clustering","title":"KMeans","text":"","category":"section"},{"location":"api.html#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api.html#Datasets","page":"API Reference","title":"Datasets","text":"","category":"section"},{"location":"api.html","page":"API Reference","title":"API Reference","text":"Modules = [NovaML.Datasets]\nOrder = [:module, :type, :function, :macro]","category":"page"},{"location":"api.html#NovaML.Datasets.load_boston-Tuple{}","page":"API Reference","title":"NovaML.Datasets.load_boston","text":"load_boston(; return_X_y=false)\n\nLoad and return the Boston house prices dataset (regression).\n\nThis function creates a synthetic version of the Boston Housing dataset for demonstration purposes, as the original dataset might not be available.\n\nArguments\n\nreturn_X_y::Bool: If true, returns (X, y) instead of a dict-like object.\n\nReturns\n\nIf return_X_y is false, returns a Dict with the following keys:\n\"data\": ndarray of shape (506, 13)   The data matrix.\n\"target\": ndarray of shape (506,)   The regression target.\n\"feature_names\": list   The names of the dataset columns.\n\"DESCR\": str   The full description of the dataset.\nIf return_X_y is true, returns a tuple (data, target):\ndata : ndarray of shape (506, 13)\ntarget : ndarray of shape (506,)\n\nDescription\n\nThe Boston Housing dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms.\n\nNote: This function generates synthetic data based on the structure of the original Boston Housing dataset. The actual values and relationships in the data are simulated and do not represent real housing data.\n\nFeatures\n\n1. CRIM: per capita crime rate by town\n2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n3. INDUS: proportion of non-retail business acres per town\n4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n5. NOX: nitric oxides concentration (parts per 10 million)\n6. RM: average number of rooms per dwelling\n7. AGE: proportion of owner-occupied units built prior to 1940\n8. DIS: weighted distances to five Boston employment centres\n9. RAD: index of accessibility to radial highways\n10. TAX: full-value property-tax rate per $10,000\n11. PTRATIO: pupil-teacher ratio by town\n12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n13. LSTAT: % lower status of the population\n\nTarget\n\n- MEDV: Median value of owner-occupied homes in $1000's\n\nExample\n\n```julia\n\nLoad the Boston Housing dataset\n\nboston = load_boston()\n\nAccess the data and target\n\nX = boston[\"data\"] y = boston[\"target\"]\n\nGet feature names\n\nfeaturenames = boston[\"featurenames\"]\n\nAlternatively, get data and target directly\n\nX, y = loadboston(returnX_y=true)\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Datasets.load_breast_cancer-Tuple{}","page":"API Reference","title":"NovaML.Datasets.load_breast_cancer","text":"load_breast_cancer(; return_X_y=false)\n\nLoad and return the Wisconsin Breast Cancer dataset (classification).\n\nArguments\n\nreturn_X_y::Bool: If true, returns (X, y) instead of a dict-like object.\n\nReturns\n\nIf return_X_y is false, returns a Dict with the following keys:\n\"data\": Matrix{Float64} of shape (569, 30)   The data matrix.\n\"target\": Vector{Bool} of shape (569,)   The classification target.\n\"feature_names\": Vector{String}   The names of the dataset columns.\n\"target_names\": Vector{String}   The names of target classes.\n\"DESCR\": String   The full description of the dataset.\nIf return_X_y is true, returns a tuple (data, target):\ndata: Matrix{Float64} of shape (569, 30)\ntarget: Vector{Bool} of shape (569,)\n\nDescription\n\nThe Wisconsin Breast Cancer dataset is a classic and very easy binary classification dataset.\n\nFeatures\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image.\n\nTen real-valued features are computed for each cell nucleus:     1) radius (mean of distances from center to points on the perimeter)     2) texture (standard deviation of gray-scale values)     3) perimeter     4) area     5) smoothness (local variation in radius lengths)     6) compactness (perimeter^2 / area - 1.0)     7) concavity (severity of concave portions of the contour)     8) concave points (number of concave portions of the contour)     9) symmetry     10) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.\n\nTarget\n\n- 0: benign\n- 1: malignant\n\nDataset Characteristics\n\n:Number of Instances: 569\n:Number of Attributes: 30 numeric, predictive attributes and the class\n:Attribute Information: 10 real-valued features are computed for each cell nucleus:\n    a) radius (mean of distances from center to points on the perimeter)\n    b) texture (standard deviation of gray-scale values)\n    c) perimeter\n    d) area\n    e) smoothness (local variation in radius lengths)\n    f) compactness (perimeter^2 / area - 1.0)\n    g) concavity (severity of concave portions of the contour)\n    h) concave points (number of concave portions of the contour)\n    i) symmetry\n    j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error, and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\n:Class Distribution: 212 Malignant, 357 Benign\n\nExample\n\n```julia\n\nLoad the Breast Cancer dataset\n\nbreastcancer = loadbreast_cancer()\n\nAccess the data and target\n\nX = breastcancer[\"data\"] y = breastcancer[\"target\"]\n\nGet feature names and target names\n\nfeaturenames = breastcancer[\"featurenames\"] targetnames = breastcancer[\"targetnames\"]\n\nAlternatively, get data and target directly\n\nX, y = loadbreastcancer(returnXy=true)\n\nNotes\n\nThis function downloads the Wisconsin Breast Cancer dataset from the UCI Machine Learning Repository if it's not already present in the local directory.\n\nThe dataset was created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian at the University of Wisconsin-Madison.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Datasets.load_iris-Tuple{}","page":"API Reference","title":"NovaML.Datasets.load_iris","text":"load_iris(; return_X_y=false)\n\nLoad and return the iris dataset (classification).\n\nArguments\n\nreturn_X_y::Bool: If true, returns (X, y) instead of a dict-like object.\n\nReturns\n\nIf return_X_y is false, returns a Dict with the following keys:\n\"data\": Matrix{Float64} of shape (150, 4)   The data matrix.\n\"target\": Vector{Int} of shape (150,)   The classification target.\n\"feature_names\": Vector{String}   The names of the dataset columns.\n\"target_names\": Vector{String}   The names of target classes.\n\"DESCR\": String   The full description of the dataset.\nIf return_X_y is true, returns a tuple (data, target):\ndata: Matrix{Float64} of shape (150, 4)\ntarget: Vector{Int} of shape (150,)\n\nDescription\n\nThe iris dataset is a classic and very easy multi-class classification dataset.\n\nFeatures\n\n1. sepal length (cm)\n2. sepal width (cm)\n3. petal length (cm)\n4. petal width (cm)\n\nTarget\n\n- Iris-setosa (1)\n- Iris-versicolor (2)\n- Iris-virginica (3)\n\nDataset Characteristics\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n:Class:\n    - Iris-Setosa\n    - Iris-Versicolour\n    - Iris-Virginica\n\nExample\n\n```julia\n\nLoad the Iris dataset\n\niris = load_iris()\n\nAccess the data and target\n\nX = iris[\"data\"] y = iris[\"target\"]\n\nGet feature names and target names\n\nfeaturenames = iris[\"featurenames\"] targetnames = iris[\"targetnames\"]\n\nAlternatively, get data and target directly\n\nX, y = loadiris(returnX_y=true)\n\nNotes\n\nThis function downloads the Iris dataset from the UCI Machine Learning Repository if it's not already present in the local directory.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Datasets.load_wine-Tuple{}","page":"API Reference","title":"NovaML.Datasets.load_wine","text":"load_wine(; return_X_y=false)\n\nLoad and return the wine dataset (classification).\n\nArguments\n\nreturn_X_y::Bool: If true, returns (X, y) instead of a dict-like object.\n\nReturns\n\nIf return_X_y is false, returns a Dict with the following keys:\n\"data\": Matrix{Float64} of shape (178, 13)   The data matrix.\n\"target\": Vector{Int} of shape (178,)   The classification target.\n\"feature_names\": Vector{String}   The names of the dataset columns.\n\"target_names\": Vector{String}   The names of target classes.\n\"DESCR\": String   The full description of the dataset.\nIf return_X_y is true, returns a tuple (data, target):\ndata: Matrix{Float64} of shape (178, 13)\ntarget: Vector{Int} of shape (178,)\n\nDescription\n\nThis dataset is a classic and very easy multi-class classification dataset.\n\nFeatures\n\n1) Alcohol\n2) Malic acid\n3) Ash\n4) Alcalinity of ash\n5) Magnesium\n6) Total phenols\n7) Flavanoids\n8) Nonflavanoid phenols\n9) Proanthocyanins\n10) Color intensity\n11) Hue\n12) OD280/OD315 of diluted wines\n13) Proline\n\nTarget\n\n- class 1 (0)\n- class 2 (1)\n- class 3 (2)\n\nDataset Characteristics\n\n:Number of Instances: 178\n:Number of Attributes: 13 numeric, predictive attributes and the class\n:Attribute Information:\n    - Alcohol\n    - Malic acid\n    - Ash\n    - Alcalinity of ash\n    - Magnesium\n    - Total phenols\n    - Flavanoids\n    - Nonflavanoid phenols\n    - Proanthocyanins\n    - Color intensity\n    - Hue\n    - OD280/OD315 of diluted wines\n    - Proline\n\n:Class:\n    - class 1\n    - class 2\n    - class 3\n\nExample\n\n```julia\n\nLoad the Wine dataset\n\nwine = load_wine()\n\nAccess the data and target\n\nX = wine[\"data\"] y = wine[\"target\"]\n\nGet feature names and target names\n\nfeaturenames = wine[\"featurenames\"] targetnames = wine[\"targetnames\"]\n\nAlternatively, get data and target directly\n\nX, y = loadwine(returnX_y=true)\n\nNotes\n\nThis function downloads the Wine dataset from the UCI Machine Learning Repository if it's not already present in the local directory.\n\nThe data set contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample.\n\nThe classes are ordered and not balanced (class 1 has 59 samples, class 2 has 71 samples, and class 3 has 48 samples).\n\nThis dataset is also excellent for visualization techniques.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Datasets.make_blobs-Tuple{}","page":"API Reference","title":"NovaML.Datasets.make_blobs","text":"make_blobs(;\n    n_samples::Union{Int, Vector{Int}} = 100,\n    n_features::Int = 2,\n    centers::Union{Int, Matrix{Float64}} = nothing,\n    cluster_std::Union{Float64, Vector{Float64}} = 1.0,\n    center_box::Tuple{Float64, Float64} = (-10.0, 10.0),\n    shuffle::Bool = true,\n    random_state::Union{Int, Nothing} = nothing,\n    return_centers::Bool = false\n)\n\nGenerate isotropic Gaussian blobs for clustering.\n\nArguments\n\nn_samples::Union{Int, Vector{Int}}: The total number of points equally divided among clusters, or the number of samples per cluster.\nn_features::Int: The number of features for each sample.\ncenters::Union{Int, Matrix{Float64}}: The number of centers to generate, or a matrix of center locations.\ncluster_std::Union{Float64, Vector{Float64}}: The standard deviation of the clusters.\ncenter_box::Tuple{Float64, Float64}: The bounding box for each cluster center when centers are generated at random.\nshuffle::Bool: Shuffle the samples.\nrandom_state::Union{Int, Nothing}: Determines random number generation for dataset creation.\nreturn_centers::Bool: If true, returns the centers in addition to X and y.\n\nReturns\n\nIf return_centers is false:\nX::Matrix{Float64}: Generated samples.\ny::Vector{Int}: The integer labels for cluster membership of each sample.\nIf return_centers is true:\nX::Matrix{Float64}: Generated samples.\ny::Vector{Int}: The integer labels for cluster membership of each sample.\ncenters::Matrix{Float64}: The centers used to generate the data.\n\nDescription\n\nThis function generates samples from isotropic Gaussian blobs for clustering. It can be used for testing clustering algorithms or as a simple dataset for demonstration purposes.\n\nExample\n\n```julia\n\nGenerate a simple dataset with 3 clusters\n\nX, y = makeblobs(nsamples=300, centers=3, nfeatures=2, randomstate=42)\n\nGenerate a dataset with specified centers and return the centers\n\ncenters = [0 0; 1 1; 2 2] X, y, centers = makeblobs(nsamples=300, centers=centers, clusterstd=0.5, returncenters=true)\n\nNotes\n\nIf centers is an int, it is interpreted as the number of centers to generate, and they are generated randomly within center_box.\n\nIf centers is a 2-d array, it is interpreted as the actual centers to use, and n_features is ignored in this case.\nIf n_samples is an int, it is interpreted as the total number of samples, which are then evenly divided among clusters.\nIf n_samples is an array, it is interpreted as the number of samples per cluster.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Datasets.make_moons-Tuple{}","page":"API Reference","title":"NovaML.Datasets.make_moons","text":"make_moons(;\n    n_samples::Union{Int, Tuple{Int, Int}}=100,\n    shuffle::Bool=true,\n    noise::Union{Float64, Nothing}=nothing,\n    random_state::Union{Int, Nothing}=nothing\n)\n\nGenerate two interleaving half circles for binary classification.\n\nArguments\n\nn_samples::Union{Int, Tuple{Int, Int}}: The total number of points generated or a tuple containing the number of points in each of the two moons.\nshuffle::Bool: Whether to shuffle the samples.\nnoise::Union{Float64, Nothing}: Standard deviation of Gaussian noise added to the data.\nrandom_state::Union{Int, Nothing}: Determines random number generation for dataset creation.\n\nReturns\n\nX::Matrix{Float64}: The generated samples, of shape (n_samples, 2).\ny::Vector{Int}: The integer labels (0 or 1) for class membership of each sample.\n\nDescription\n\nThis function generates a binary classification dataset in the shape of two interleaving half moons. It can be used for testing classification algorithms or as a simple dataset for demonstration purposes.\n\nExample\n\n```julia\n\nGenerate a simple moon dataset\n\nX, y = makemoons(nsamples=100, noise=0.1, random_state=42)\n\nGenerate a moon dataset with different number of samples in each moon\n\nX, y = makemoons(nsamples=(60, 40), noise=0.1, shuffle=false)\n\nNotes\n\nIf n_samples is an integer, it generates approximately equal numbers of samples in each moon.\nIf the number is odd, the extra sample is added to the first moon.\nIf n_samples is a tuple of two integers, it specifies the number of samples for each moon respectively.\nThe two moons are generated on a 2D plane. The first moon is a half circle of radius 1 centered at (0, 0),\n\nwhile the second moon is a half circle of radius 1 centered at (1, 0.5).\n\nIf noise is specified, Gaussian noise with standard deviation noise is added to the data.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Clustering","page":"API Reference","title":"Clustering","text":"","category":"section"},{"location":"api.html","page":"API Reference","title":"API Reference","text":"Modules = [NovaML.Cluster]\nOrder = [:module, :type, :function, :macro]","category":"page"},{"location":"api.html#NovaML.Cluster.AgglomerativeClustering","page":"API Reference","title":"NovaML.Cluster.AgglomerativeClustering","text":"AgglomerativeClustering\n\nA struct representing Agglomerative Clustering, a hierarchical clustering algorithm.\n\nFields\n\nn_clusters::Union{Int, Nothing}: The number of clusters to find. If nothing, it must be used with distance_threshold.\nmetric::Union{String, Function}: The metric to use for distance computation. Can be \"euclidean\", \"manhattan\", or a custom function.\nmemory::Union{String, Nothing}: Used to cache the distance matrix between iterations.\nconnectivity::Union{AbstractMatrix, Function, Nothing}: Connectivity matrix or callable to be used.\ncompute_full_tree::Union{Bool, String}: Whether to compute the full tree or stop early.\nlinkage::String: The linkage criterion to use. Can be \"ward\", \"complete\", \"average\", or \"single\".\ndistance_threshold::Union{Float64, Nothing}: The threshold to stop clustering.\ncompute_distances::Bool: Whether to compute distances.\n\nFitted Attributes\n\nlabels_::Vector{Int}: Cluster labels for each point.\nn_leaves_::Int: Number of leaves in the hierarchical tree.\nn_connected_components_::Int: Number of connected components in the graph.\nchildren_::Matrix{Int}: The children of each non-leaf node.\ndistances_::Vector{Float64}: Distances between nodes in the tree.\n\nConstructor\n\nAgglomerativeClustering(;\n    n_clusters::Union{Int, Nothing}=2,\n    metric::Union{String, Function}=\"euclidean\",\n    memory::Union{String, Nothing}=nothing,\n    connectivity::Union{AbstractMatrix, Function, Nothing}=nothing,\n    compute_full_tree::Union{Bool, String}=\"auto\",\n    linkage::String=\"ward\",\n    distance_threshold::Union{Float64, Nothing}=nothing,\n    compute_distances::Bool=false\n)\n\nConstructs an AgglomerativeClustering object with the specified parameters.\n\nExamples\n\n# Create an AgglomerativeClustering object with 3 clusters\nclustering = AgglomerativeClustering(n_clusters=3)\n\n# Create an AgglomerativeClustering object with a distance threshold\nclustering = AgglomerativeClustering(distance_threshold=1.5, linkage=\"single\")\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Cluster.AgglomerativeClustering-Tuple{AbstractMatrix, Symbol}","page":"API Reference","title":"NovaML.Cluster.AgglomerativeClustering","text":"(clustering::AgglomerativeClustering)(X::AbstractMatrix, type::Symbol) Fit the clustering model and return the cluster labels.\n\nArguments\n\nX::AbstractMatrix: The input data matrix. type::Symbol: Must be :fit_predict to fit the model and return labels.\n\nReturns\n\nlabels::Vector{Int}: The cluster labels for each input sample.\n\nExamples\n\nX = rand(100, 5)\nclustering = AgglomerativeClustering(n_clusters=3)\nlabels = clustering(X, :fit_predict)\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.AgglomerativeClustering-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Cluster.AgglomerativeClustering","text":"(clustering::AgglomerativeClustering)(X::AbstractMatrix; y=nothing)\n\nPerform agglomerative clustering on the input data.\n\nArguments\n\nX::AbstractMatrix: The input data matrix where each row is a sample and each column is a feature. y=nothing: Ignored. Present for API consistency.\n\nReturns\n\nclustering::AgglomerativeClustering: The fitted clustering object.\n\n#Examples\n\nX = rand(100, 5)  # 100 samples, 5 features\nclustering = AgglomerativeClustering(n_clusters=3)\nfitted_clustering = clustering(X)\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.DBSCAN","page":"API Reference","title":"NovaML.Cluster.DBSCAN","text":"(dbscan::DBSCAN)(X::AbstractMatrix, y=nothing; sample_weight=nothing)\n\nPerform DBSCAN clustering on the input data.\n\nArguments\n\nX::AbstractMatrix: The input data matrix where each row is a sample and each column is a feature. y=nothing: Ignored. Present for API consistency. sample_weight=nothing: Weight of each sample, used in computing the number of neighbors within eps.\n\nReturns\n\ndbscan::DBSCAN: The fitted DBSCAN object.\n\nExamples\n\nX = rand(100, 5)  # 100 samples, 5 features\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nfitted_dbscan = dbscan(X)\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Cluster.DBSCAN-2","page":"API Reference","title":"NovaML.Cluster.DBSCAN","text":"DBSCAN\n\nA struct representing the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering algorithm.\n\nFields\n\neps::Float64: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\nmin_samples::Int: The number of samples in a neighborhood for a point to be considered as a core point.\nmetric::Union{String, Metric}: The metric to use when calculating distance between instances.\nmetric_params::Union{Nothing, Dict}: Additional keyword arguments for the metric function.\nalgorithm::Symbol: The algorithm to be used by the NearestNeighbors module.\nleaf_size::Int: Leaf size passed to BallTree or KDTree.\np::Union{Nothing, Float64}: The power of the Minkowski metric to be used to calculate distance between points.\nn_jobs::Union{Nothing, Int}: The number of parallel jobs to run.\n\nFitted Attributes\n\ncore_sample_indices_::Vector{Int}: Indices of core samples.\ncomponents_::Matrix{Float64}: Copy of each core sample found by training.\nlabels_::Vector{Int}: Cluster labels for each point in the dataset given to fit().\nn_features_in_::Int: Number of features seen during fit.\nfeature_names_in_::Vector{String}: Names of features seen during fit.\nfitted::Bool: Whether the model has been fitted.\n\nConstructor\n\nDBSCAN(;\n    eps::Float64 = 0.5,\n    min_samples::Int = 5,\n    metric::Union{String, Metric} = \"euclidean\",\n    metric_params::Union{Nothing, Dict} = nothing,\n    algorithm::Symbol = :auto,\n    leaf_size::Int = 30,\n    p::Union{Nothing, Float64} = nothing,\n    n_jobs::Union{Nothing, Int} = nothing\n)\n\nConstructs a DBSCAN object with the specified parameters.\n\nExamples\n\n```julia\n\nCreate a DBSCAN object with default parameters\n\ndbscan = DBSCAN()\n\nCreate a DBSCAN object with custom parameters\n\ndbscan = DBSCAN(eps=0.7, min_samples=10, metric=\"manhattan\")\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Cluster.KMeans","page":"API Reference","title":"NovaML.Cluster.KMeans","text":"KMeans <: AbstractModel\n\nRepresents the K-Means clustering algorithm.\n\nFields\n\nn_clusters::Int: The number of clusters to form.\ninit::Union{String, Matrix{Float64}, Function}: Method for initialization.\nn_init::Union{Int, String}: Number of time the k-means algorithm will be run with different centroid seeds.\nmax_iter::Int: Maximum number of iterations of the k-means algorithm for a single run.\ntol::Float64: Relative tolerance with regards to inertia to declare convergence.\nverbose::Int: Verbosity mode.\nrandom_state::Union{Int, Nothing}: Determines random number generation for centroid initialization.\ncopy_x::Bool: When pre-computing distances it is more numerically accurate to center the data first.\nalgorithm::String: K-means algorithm to use.\n\nFitted Attributes\n\ncluster_centers_::Union{Matrix{Float64}, Nothing}: Coordinates of cluster centers.\nlabels_::Union{Vector{Int}, Nothing}: Labels of each point.\ninertia_::Union{Float64, Nothing}: Sum of squared distances of samples to their closest cluster center.\nn_iter_::Union{Int, Nothing}: Number of iterations run.\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Cluster.KMeans-2","page":"API Reference","title":"NovaML.Cluster.KMeans","text":"(kmeans::KMeans)(X::AbstractVecOrMat{Float64}, y=nothing; sample_weight=nothing)\n\nCompute k-means clustering.\n\nArguments\n\nX::AbstractVecOrMat{Float64}: Training instances to cluster.\ny: Ignored. Not used, present for API consistency by convention.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nIf the model is not fitted, returns the fitted model.\nIf the model is already fitted, returns the predicted labels for X.\n\n\n\n\n\n","category":"type"},{"location":"api.html#Base.show-Tuple{IO, DBSCAN}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, dbscan::DBSCAN) Custom show method for DBSCAN objects.\n\nArguments\n\nio::IO: The I/O stream to which the representation is written. dbscan::DBSCAN: The DBSCAN object to be displayed.\n\nExamples\n\ndbscan = DBSCAN(eps=0.7, min_samples=10)\nprintln(dbscan)\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, KMeans}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, kmeans::KMeans)\n\nCustom show method for KMeans instances.\n\nArguments\n\nio::IO: The I/O stream.\nkmeans::KMeans: The KMeans instance to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.assign_labels-Tuple{AbstractMatrix{Float64}, Matrix{Float64}}","page":"API Reference","title":"NovaML.Cluster.assign_labels","text":"assign_labels(X::AbstractMatrix{Float64}, centroids::Matrix{Float64})\n\nAssign labels to data points based on the nearest centroid.\n\nArguments\n\nX::AbstractMatrix{Float64}: The input data.\ncentroids::Matrix{Float64}: The current centroids.\n\nReturns\n\nVector{Int}: The assigned labels for each data point.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.compute_distances-Tuple{AbstractMatrix, Union{Function, String}}","page":"API Reference","title":"NovaML.Cluster.compute_distances","text":"compute_distances(X::AbstractMatrix, metric::Union{String, Function})\n\nCompute the distance matrix for the input data using the specified metric.\n\nArguments\n\nX::AbstractMatrix: The input data matrix. metric::Union{String, Function}: The distance metric to use. Can be \"euclidean\", \"manhattan\", or a custom function.\n\nReturns\n\ndistances::Matrix: The computed distance matrix.\n\nExamples\n\nX = rand(10, 3)\ndistances = compute_distances(X, \"euclidean\")\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.compute_inertia","page":"API Reference","title":"NovaML.Cluster.compute_inertia","text":"compute_inertia(X::Matrix{Float64}, centroids::Matrix{Float64}, labels::Vector{Int}, sample_weight=nothing)\n\nCompute the inertia, the sum of squared distances of samples to their closest cluster center.\n\nArguments\n\nX::Matrix{Float64}: The input data.\ncentroids::Matrix{Float64}: The current centroids.\nlabels::Vector{Int}: The current label assignments.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nFloat64: The computed inertia.\n\n\n\n\n\n","category":"function"},{"location":"api.html#NovaML.Cluster.fit_predict","page":"API Reference","title":"NovaML.Cluster.fit_predict","text":"fit_predict(kmeans::KMeans, X::Matrix{Float64}, y=nothing; sample_weight=nothing)\n\nCompute cluster centers and predict cluster index for each sample.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nX::Matrix{Float64}: New data to transform.\ny: Ignored.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nVector{Int}: Index of the cluster each sample belongs to.\n\n\n\n\n\n","category":"function"},{"location":"api.html#NovaML.Cluster.fit_transform","page":"API Reference","title":"NovaML.Cluster.fit_transform","text":"fit_transform(kmeans::KMeans, X::Matrix{Float64}, y=nothing; sample_weight=nothing)\n\nCompute clustering and transform X to cluster-distance space.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nX::Matrix{Float64}: New data to transform.\ny: Ignored.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nMatrix{Float64}: X transformed in the new space.\n\n\n\n\n\n","category":"function"},{"location":"api.html#NovaML.Cluster.get_params-Tuple{DBSCAN}","page":"API Reference","title":"NovaML.Cluster.get_params","text":"get_params(dbscan::DBSCAN) Get parameters for this estimator.\n\nReturns\n\nparams::Dict: Parameter names mapped to their values.\n\nExamples\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.get_params-Tuple{KMeans}","page":"API Reference","title":"NovaML.Cluster.get_params","text":"get_params(kmeans::KMeans; deep=true)\n\nGet parameters for this estimator.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\ndeep::Bool: If True, will return the parameters for this estimator and contained subobjects that are estimators.\n\nReturns\n\nDict: Parameter names mapped to their values.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.initialize_centroids-Tuple{KMeans, Matrix{Float64}}","page":"API Reference","title":"NovaML.Cluster.initialize_centroids","text":"initialize_centroids(kmeans::KMeans, X::Matrix{Float64})\n\nInitialize the centroids for K-Means clustering.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nX::Matrix{Float64}: The input data.\n\nReturns\n\nMatrix{Float64}: The initial centroids.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.kmeans_plus_plus-Tuple{Matrix{Float64}, Int64}","page":"API Reference","title":"NovaML.Cluster.kmeans_plus_plus","text":"kmeans_plus_plus(X::Matrix{Float64}, n_clusters::Int)\n\nPerform K-Means++ initialization.\n\nArguments\n\nX::Matrix{Float64}: The input data.\nn_clusters::Int: The number of clusters.\n\nReturns\n\nMatrix{Float64}: The initial centroids chosen by K-Means++.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.score","page":"API Reference","title":"NovaML.Cluster.score","text":"score(kmeans::KMeans, X::Matrix{Float64}, y=nothing; sample_weight=nothing)\n\nOpposite of the value of X on the K-means objective.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nX::Matrix{Float64}: New data.\ny: Ignored.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nFloat64: Opposite of the value of X on the K-means objective.\n\n\n\n\n\n","category":"function"},{"location":"api.html#NovaML.Cluster.set_params!-Tuple{DBSCAN}","page":"API Reference","title":"NovaML.Cluster.set_params!","text":"set_params!(dbscan::DBSCAN; kwargs...) Set the parameters of this estimator.\n\nArguments\n\nkwargs...: Estimator parameters.\n\nReturns\n\ndbscan::DBSCAN: The DBSCAN object.\n\nExamples\n\ndbscan = DBSCAN()\nset_params!(dbscan, eps=0.8, min_samples=15)\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.set_params!-Tuple{KMeans}","page":"API Reference","title":"NovaML.Cluster.set_params!","text":"set_params!(kmeans::KMeans; params...)\n\nSet the parameters of this estimator.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nparams...: Estimator parameters.\n\nReturns\n\nKMeans: The estimator instance.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.transform-Tuple{KMeans, Matrix{Float64}}","page":"API Reference","title":"NovaML.Cluster.transform","text":"transform(kmeans::KMeans, X::Matrix{Float64})\n\nTransform X to a cluster-distance space.\n\nArguments\n\nkmeans::KMeans: The KMeans instance.\nX::Matrix{Float64}: New data to transform.\n\nReturns\n\nMatrix{Float64}: X transformed in the new space.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Cluster.update_centroids","page":"API Reference","title":"NovaML.Cluster.update_centroids","text":"update_centroids(X::Matrix{Float64}, labels::Vector{Int}, n_clusters::Int, sample_weight=nothing)\n\nUpdate the centroids based on the current label assignments.\n\nArguments\n\nX::Matrix{Float64}: The input data.\nlabels::Vector{Int}: The current label assignments.\nn_clusters::Int: The number of clusters.\nsample_weight: The weights for each observation in X.\n\nReturns\n\nMatrix{Float64}: The updated centroids.\n\n\n\n\n\n","category":"function"},{"location":"api.html#Decomposition","page":"API Reference","title":"Decomposition","text":"","category":"section"},{"location":"api.html","page":"API Reference","title":"API Reference","text":"Modules = [NovaML.Decomposition]\nOrder = [:module, :type, :function, :macro]","category":"page"},{"location":"api.html#NovaML.Decomposition.LatentDirichletAllocation","page":"API Reference","title":"NovaML.Decomposition.LatentDirichletAllocation","text":"LatentDirichletAllocation\n\nLatent Dirichlet Allocation (LDA) with online variational Bayes algorithm.\n\nLDA is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\n\nFields\n\nn_components::Int: Number of topics.\ndoc_topic_prior::Union{Float64, Nothing}: Prior of document topic distribution.\ntopic_word_prior::Union{Float64, Nothing}: Prior of topic word distribution.\nlearning_method::Symbol: Method used to update the model: :batch for batch learning, :online for online learning.\nlearning_decay::Float64: It is a parameter that control the rate at which the learning rate decreases.\nlearning_offset::Float64: A (positive) parameter that downweights early iterations in online learning.\nmax_iter::Int: The maximum number of iterations.\nbatch_size::Int: Number of documents to use in each EM iteration in online learning method.\nevaluate_every::Int: How often to evaluate perplexity.\ntotal_samples::Float64: Total number of documents.\nperp_tol::Float64: Perplexity tolerance in batch learning.\nmean_change_tol::Float64: Stopping tolerance for updating document topic distribution in E-step.\nmax_doc_update_iter::Int: Max number of iterations for updating document topic distribution in E-step.\nn_jobs::Union{Int, Nothing}: The number of jobs to use in the E-step.\nverbose::Int: Verbosity level.\nrandom_state::Union{Int, Nothing}: Seed for random number generation.\n\nLearned attributes\n\ncomponents_::Union{Matrix{Float64}, Nothing}: Topic word distribution. shape = (ncomponents, nfeatures)\nexp_dirichlet_component_::Union{Matrix{Float64}, Nothing}: Exponential value of expectation of log topic word distribution. shape = (ncomponents, nfeatures)\nn_batch_iter_::Int: Number of iterations of the EM step.\nn_iter_::Int: Number of passes over the dataset.\nbound_::Float64: Final perplexity score on training set.\nn_features_in_::Int: Number of features seen during fit.\nfeature_names_in_::Union{Vector{String}, Nothing}: Names of features seen during fit.\n\nExample\n\n```julia using NovaML\n\nCreate an LDA model\n\nlda = LatentDirichletAllocation(ncomponents=10, randomstate=42)\n\nFit the model to data\n\ndoctopicdistr = lda(X)\n\nTransform new data\n\nnewdoctopicdistr = lda(newX)\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Decomposition.LatentDirichletAllocation-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition.LatentDirichletAllocation","text":"(lda::LatentDirichletAllocation)(X::AbstractMatrix{T}; type=nothing) where T <: Real\n\nFit the model to X, or transform X if the model is already fitted.\n\nArguments\n\nX::AbstractMatrix{T}: Document-term matrix.\ntype: Ignored. Present for API consistency.\n\nReturns\n\nIf the model is not fitted, returns the document-topic distribution after fitting.\nIf the model is already fitted, returns the document-topic distribution for X.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition.PCA","page":"API Reference","title":"NovaML.Decomposition.PCA","text":"PCA\n\nPrincipal Component Analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n\nFields\n\nn_components::Union{Int, Float64, String, Nothing}: Number of components to keep.\nwhiten::Bool: When True, the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\nfitted::Bool: Whether the PCA model has been fitted to data.\n\nFitted Attributes\n\ncomponents_::Union{Matrix{Float64}, Nothing}: Principal axes in feature space, representing the directions of maximum variance in the data.\nexplained_variance_::Union{Vector{Float64}, Nothing}: The amount of variance explained by each of the selected components.\nexplained_variance_ratio_::Union{Vector{Float64}, Nothing}: Percentage of variance explained by each of the selected components.\nsingular_values_::Union{Vector{Float64}, Nothing}: The singular values corresponding to each of the selected components.\nmean_::Union{Vector{Float64}, Nothing}: Per-feature empirical mean, estimated from the training set.\nn_samples_::Union{Int, Nothing}: Number of samples in the training data.\nn_features_::Union{Int, Nothing}: Number of features in the training data.\nn_components_::Union{Int, Nothing}: The estimated number of components.\nnoise_variance_::Union{Float64, Nothing}: The estimated noise covariance following the Probabilistic PCA model.\n\nExample\n\npca = PCA(n_components=2)\nX_transformed = pca(X)\nX_inverse = pca(X_transformed, :inverse_transform)\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Decomposition.PCA-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition.PCA","text":"(pca::PCA)(X::AbstractMatrix{T}) where T <: Real\n\nFit the model with X and apply the dimensionality reduction on X.\n\nArguments\n\nX::AbstractMatrix{T}: Training data, where nsamples is the number of samples and nfeatures is the number of features.\n\nReturns\n\nMatrix{Float64}: Transformed values.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition.PCA-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Symbol}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition.PCA","text":"(pca::PCA)(X::AbstractMatrix{T}, mode::Symbol) where T <: Real\n\nTransform data back to its original space.\n\nArguments\n\nX::AbstractMatrix{T}: New data, where nsamples is the number of samples and ncomponents is the number of components.\nmode::Symbol: Must be :inverse_transform.\n\nReturns\n\nMatrix{Float64}: X_original array.\n\nThrows\n\nErrorException: If mode is not :inverse_transform.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, LatentDirichletAllocation}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, lda::LatentDirichletAllocation)\n\nCustom show method for LatentDirichletAllocation.\n\nArguments\n\nio::IO: The I/O stream\nlda::LatentDirichletAllocation: The LDA model to display\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, PCA}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, pca::PCA)\n\nCustom show method for PCA.\n\nArguments\n\nio::IO: The I/O stream.\npca::PCA: The PCA model to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._e_step-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._e_step","text":"_e_step(lda::LatentDirichletAllocation, X::AbstractMatrix{T}) where T <: Real\n\nE-step in EM update.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\n\nReturns\n\nMatrix{Float64}: Document-topic distribution.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._fit_batch-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._fit_batch","text":"_fit_batch(lda::LatentDirichletAllocation, X::AbstractMatrix{T}) where T <: Real\n\nFit the model to X using batch variational Bayes method.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\n\nReturns\n\nMatrix{Float64}: Document-topic distribution.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._fit_online-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._fit_online","text":"_fit_online(lda::LatentDirichletAllocation, X::AbstractMatrix{T}) where T <: Real\n\nFit the model to X using online variational Bayes method.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\n\nReturns\n\nMatrix{Float64}: Document-topic distribution.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._fit_transform-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._fit_transform","text":"_fit_transform(lda::LatentDirichletAllocation, X::AbstractMatrix{T}) where T <: Real\n\nFit the model to X and return the document-topic distribution.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\n\nReturns\n\nMatrix{Float64}: Document-topic distribution.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._m_step-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}, Matrix{Float64}}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}, Matrix{Float64}, Float64}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._m_step","text":"_m_step(lda::LatentDirichletAllocation, X::AbstractMatrix{T}, doc_topic_distr::Matrix{Float64}, scale::Float64=1.0) where T <: Real\n\nM-step in EM update.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\ndoc_topic_distr::Matrix{Float64}: Document-topic distribution.\nscale::Float64: Scaling factor for online update.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._perplexity-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}, Matrix{Float64}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._perplexity","text":"_perplexity(lda::LatentDirichletAllocation, X::AbstractMatrix{T}, doc_topic_distr::Matrix{Float64}) where T <: Real\n\nCalculate approximate perplexity for data X.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\ndoc_topic_distr::Matrix{Float64}: Document-topic distribution.\n\nReturns\n\nFloat64: The calculated bound.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Decomposition._transform-Union{Tuple{T}, Tuple{LatentDirichletAllocation, AbstractMatrix{T}}} where T<:Real","page":"API Reference","title":"NovaML.Decomposition._transform","text":"_transform(lda::LatentDirichletAllocation, X::AbstractMatrix{T}) where T <: Real\n\nTransform X to document-topic distribution.\n\nArguments\n\nlda::LatentDirichletAllocation: The LDA model.\nX::AbstractMatrix{T}: Document-term matrix.\n\nReturns\n\nMatrix{Float64}: Document-topic distribution.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Ensemble-Methods","page":"API Reference","title":"Ensemble Methods","text":"","category":"section"},{"location":"api.html","page":"API Reference","title":"API Reference","text":"Modules = [NovaML.Ensemble]\nOrder = [:module, :type, :function, :macro]","category":"page"},{"location":"api.html#NovaML.Ensemble.AdaBoostClassifier","page":"API Reference","title":"NovaML.Ensemble.AdaBoostClassifier","text":"AdaBoostClassifier <: AbstractModel\n\nAn AdaBoost classifier.\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n\nFields\n\nbase_estimator::Any: The base estimator from which the boosted ensemble is built.\nn_estimators::Int: The maximum number of estimators at which boosting is terminated.\nlearning_rate::Float64: Weight applied to each classifier at each boosting iteration.\nalgorithm::Symbol: The SAMME algorithm to use when fitting the model.\nrandom_state::Union{Int, Nothing}: Controls the random seed given at each base_estimator at each boosting iteration.\n\nFitted Attributes\n\nestimators_::Vector{Any}: The collection of fitted sub-estimators.\nestimator_weights_::Vector{Float64}: Weights for each estimator in the boosted ensemble.\nestimator_errors_::Vector{Float64}: Classification error for each estimator in the boosted ensemble.\nclasses_::Vector{Any}: The classes labels.\nn_classes_::Int: The number of classes.\nfeature_importances_::Union{Vector{Float64}, Nothing}: The feature importances if supported by the base_estimator.\nfitted::Bool: Whether the model has been fitted.\n\nExample\n\nmodel = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\nmodel(X, y)  # Fit the model\npredictions = model(X_test)  # Make predictions\nprobabilities = model(X_test, type=:probs)  # Get probability estimates\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.AdaBoostClassifier-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.AdaBoostClassifier","text":"(model::AdaBoostClassifier)(X::AbstractMatrix, y::AbstractVector) Fit the AdaBoost model.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values (class labels).\n\nReturns\n\nAdaBoostClassifier: The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.AdaBoostClassifier-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.AdaBoostClassifier","text":"(model::AdaBoostClassifier)(X::AbstractMatrix; type=nothing)\n\nPredict using the AdaBoost model.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ntype: If set to :probs, return probability estimates for each class.\n\nReturns\n\nIf type is :probs, returns probabilities of each class.\nOtherwise, returns predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.BaggingClassifier","page":"API Reference","title":"NovaML.Ensemble.BaggingClassifier","text":"BaggingClassifier <: AbstractModel\n\nA Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nFields\n\nbase_estimator::AbstractModel: The base estimator to fit on random subsets of the dataset.\nn_estimators::Int: The number of base estimators in the ensemble.\nmax_samples::Union{Int, Float64}: The number of samples to draw from X to train each base estimator.\nmax_features::Union{Int, Float64}: The number of features to draw from X to train each base estimator.\nbootstrap::Bool: Whether samples are drawn with replacement.\nbootstrap_features::Bool: Whether features are drawn with replacement.\noob_score::Bool: Whether to use out-of-bag samples to estimate the generalization error.\nwarm_start::Bool: When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.\nrandom_state::Union{Int, Nothing}: Controls the random resampling of the original dataset.\nverbose::Int: Controls the verbosity when fitting and predicting.\n\nFitted Attributes\n\nestimators_::Vector{AbstractModel}: The collection of fitted base estimators.\nestimators_features_::Vector{Vector{Int}}: The subset of drawn features for each base estimator.\nclasses_::Vector: The classes labels.\nn_classes_::Int: The number of classes.\noob_score_::Union{Float64, Nothing}: Score of the training dataset obtained using an out-of-bag estimate.\noob_decision_function_::Union{Matrix{Float64}, Nothing}: Decision function computed with out-of-bag estimate on the training set.\nfitted::Bool: Whether the model has been fitted.\n\nExample\n\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\nmodel(X, y)  # Fit the model\npredictions = model(X_test)  # Make predictions\nprobabilities = model(X_test, type=:probs)  # Get probability estimates\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.BaggingClassifier-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.BaggingClassifier","text":"(bc::BaggingClassifier)(X::AbstractMatrix, y::AbstractVector)\n\nFit the Bagging classifier.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values (class labels).\n\nReturns\n\nBaggingClassifier: The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.BaggingClassifier-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.BaggingClassifier","text":"(bc::BaggingClassifier)(X::AbstractMatrix; type=nothing)\n\nPredict class for X.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ntype: If set to :probs, return probability estimates for each class.\n\nReturns\n\nIf type is :probs, returns probabilities of each class.\nOtherwise, returns predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.GradientBoostingClassifier","page":"API Reference","title":"NovaML.Ensemble.GradientBoostingClassifier","text":"GradientBoostingClassifier <: AbstractModel\n\nGradient Boosting for classification.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n\nFields\n\nloss::String: The loss function to be optimized.\nlearning_rate::Float64: Learning rate shrinks the contribution of each tree by learning_rate.\nn_estimators::Int: The number of boosting stages to perform.\nsubsample::Float64: The fraction of samples to be used for fitting the individual base learners.\ncriterion::String: The function to measure the quality of a split.\nmin_samples_split::Union{Int, Float64}: The minimum number of samples required to split an internal node.\nmin_samples_leaf::Union{Int, Float64}: The minimum number of samples required to be at a leaf node.\nmin_weight_fraction_leaf::Float64: The minimum weighted fraction of the sum total of weights required to be at a leaf node.\nmax_depth::Union{Int, Nothing}: Maximum depth of the individual regression estimators.\nmin_impurity_decrease::Float64: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\ninit::Union{AbstractModel, String, Nothing}: An estimator object that is used to compute the initial predictions.\nrandom_state::Union{Int, Nothing}: Controls the random seed given at each tree_estimator at each boosting iteration.\nmax_features::Union{Int, Float64, String, Nothing}: The number of features to consider when looking for the best split.\nverbose::Int: Enable verbose output.\nmax_leaf_nodes::Union{Int, Nothing}: Grow trees with max_leaf_nodes in best-first fashion.\nwarm_start::Bool: When set to true, reuse the solution of the previous call to fit and add more estimators to the ensemble.\nvalidation_fraction::Float64: The proportion of training data to set aside as validation set for early stopping.\nn_iter_no_change::Union{Int, Nothing}: Used to decide if early stopping will be used to terminate training when validation score is not improving.\ntol::Float64: Tolerance for the early stopping.\nccp_alpha::Float64: Complexity parameter used for Minimal Cost-Complexity Pruning.\n\nFitted Attributes\n\nestimators_::Vector{Vector{DecisionTreeRegressor}}: The collection of fitted sub-estimators.\nclasses_::Vector: The classes labels.\nn_classes_::Int: The number of classes.\nfeature_importances_::Union{Vector{Float64}, Nothing}: The feature importances.\noob_improvement_::Union{Vector{Float64}, Nothing}: The improvement in loss on the out-of-bag samples relative to the previous iteration.\ntrain_score_::Vector{Float64}: The i-th score train_score_[i] is the loss of the model at iteration i on the in-bag sample.\nn_estimators_::Int: The number of estimators as selected by early stopping.\ninit_::Union{AbstractModel, Nothing}: The estimator that provides the initial predictions.\nfitted::Bool: Whether the model has been fitted.\n\nExample\n\nmodel = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\nmodel(X, y)  # Fit the model\npredictions = model(X_test)  # Make predictions\nprobabilities = model(X_test, type=:probs)  # Get probability estimates\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.GradientBoostingClassifier-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.GradientBoostingClassifier","text":"(gbm::GradientBoostingClassifier)(X::AbstractMatrix, y::AbstractVector)\n\nFit the gradient boosting model.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values (class labels).\n\nReturns\n\nGradientBoostingClassifier: The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.GradientBoostingClassifier-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.GradientBoostingClassifier","text":"(gbm::GradientBoostingClassifier)(X::AbstractMatrix; type=nothing)\n\nPredict class for X.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ntype: If set to :probs, return probability estimates for each class.\n\nReturns\n\nIf type is :probs, returns probabilities of each class.\nOtherwise, returns predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.InitialEstimator","page":"API Reference","title":"NovaML.Ensemble.InitialEstimator","text":"InitialEstimator <: AbstractModel\n\nAn initial estimator that always predicts a constant probability.\n\nFields\n\nprob::Float64: The constant probability to predict.\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.InitialEstimator-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.InitialEstimator","text":"(estimator::InitialEstimator)(X::AbstractMatrix)\n\nPredict using the initial estimator.\n\nArguments\n\nX::AbstractMatrix: The input samples.\n\nReturns\n\nVector{Float64}: The predictions.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.RandomForestClassifier","page":"API Reference","title":"NovaML.Ensemble.RandomForestClassifier","text":"RandomForestClassifier <: AbstractModel\n\nA random forest classifier.\n\nRandom forests are an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees.\n\nFields\n\nn_estimators::Int: The number of trees in the forest.\nmax_depth::Union{Int, Nothing}: The maximum depth of the tree.\nmin_samples_split::Int: The minimum number of samples required to split an internal node.\nmin_samples_leaf::Int: The minimum number of samples required to be at a leaf node.\nmax_features::Union{Int, Float64, String, Nothing}: The number of features to consider when looking for the best split.\nbootstrap::Bool: Whether bootstrap samples are used when building trees.\nrandom_state::Union{Int, Nothing}: Controls both the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node.\ntrees::Vector{DecisionTreeClassifier}: The collection of fitted sub-estimators.\nn_classes::Int: The number of classes.\nclasses::Vector: The class labels.\nfitted::Bool: Whether the model has been fitted.\nfeature_importances_::Union{Vector{Float64}, Nothing}: The feature importances.\nn_features::Int: The number of features when fitting the model.\n\nExample\n\n```julia rf = RandomForestClassifier(nestimators=100, maxdepth=10) rf(X, y)  # Fit the model predictions = rf(X_test)  # Make predictions\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.RandomForestClassifier-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.RandomForestClassifier","text":"(forest::RandomForestClassifier)(X::AbstractMatrix, y::AbstractVector)\n\nFit the random forest classifier.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values (class labels).\n\nReturns\n\nRandomForestClassifier: The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.RandomForestClassifier-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.RandomForestClassifier","text":"(forest::RandomForestClassifier)(X::AbstractMatrix)\n\nPredict class for X.\n\nArguments\n\nX::AbstractMatrix: The input samples.\n\nReturns\n\nVector: The predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.RandomForestRegressor","page":"API Reference","title":"NovaML.Ensemble.RandomForestRegressor","text":"RandomForestRegressor <: AbstractModel\n\nA random forest regressor.\n\nRandom forests are an ensemble learning method for regression that operate by constructing a multitude of decision trees at training time and outputting the mean prediction of the individual trees.\n\nFields\n\nn_estimators::Int: The number of trees in the forest.\ncriterion::String: The function to measure the quality of a split.\nmax_depth::Union{Int, Nothing}: The maximum depth of the tree.\nmin_samples_split::Int: The minimum number of samples required to split an internal node.\nmin_samples_leaf::Int: The minimum number of samples required to be at a leaf node.\nmin_weight_fraction_leaf::Float64: The minimum weighted fraction of the sum total of weights required to be at a leaf node.\nmax_features::Union{Int, Float64, String, Nothing}: The number of features to consider when looking for the best split.\nmax_leaf_nodes::Union{Int, Nothing}: Grow trees with maxleafnodes in best-first fashion.\nmin_impurity_decrease::Float64: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\nbootstrap::Bool: Whether bootstrap samples are used when building trees.\noob_score::Bool: Whether to use out-of-bag samples to estimate the generalization score.\nn_jobs::Union{Int, Nothing}: The number of jobs to run in parallel.\nrandom_state::Union{Int, Nothing}: Controls both the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node.\nverbose::Int: Controls the verbosity when fitting and predicting.\nwarm_start::Bool: When set to true, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.\nccp_alpha::Float64: Complexity parameter used for Minimal Cost-Complexity Pruning.\nmax_samples::Union{Int, Float64, Nothing}: If bootstrap is True, the number of samples to draw from X to train each base estimator.\n\nExample\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=10)\nrf(X, y)  # Fit the model\npredictions = rf(X_test)  # Make predictions\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.RandomForestRegressor-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.RandomForestRegressor","text":"(forest::RandomForestRegressor)(X::AbstractMatrix, y::AbstractVector)\n\nFit the random forest regressor.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values.\n\nReturns\n\nRandomForestRegressor: The fitted model.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.RandomForestRegressor-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.RandomForestRegressor","text":"(forest::RandomForestRegressor)(X::AbstractMatrix)\n\nPredict regression target for X.\n\nArguments\n\nX::AbstractMatrix: The input samples.\n\nReturns\n\nVector{Float64}: The predicted values.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.VotingClassifier","page":"API Reference","title":"NovaML.Ensemble.VotingClassifier","text":"VotingClassifier <: AbstractModel\n\nA Voting Classifier for combining multiple machine learning classifiers.\n\nThis classifier combines a number of estimators to create a single classifier that makes predictions based on either hard voting (majority vote) or soft voting (weighted average of predicted probabilities).\n\nFields\n\nestimators::Vector{Tuple{String, Any}}: List of (name, estimator) tuples.\nvoting::Symbol: The voting strategy, either :hard for majority voting or :soft for probability voting.\nweights::Union{Vector{Float64}, Nothing}: Sequence of weights for each estimator in soft voting.\nflatten_transform::Bool: Affects the shape of transform output.\nverbose::Bool: If true, prints progress messages during fitting.\n\nFitted Attributes\n\nestimators_::Vector{Any}: The fitted estimators.\nclasses_::Vector{Any}: The class labels.\nfitted::Bool: Whether the classifier is fitted.\n\nExample\n\nestimators = [(\"lr\", LogisticRegression()), (\"rf\", RandomForestClassifier())]\nvc = VotingClassifier(estimators=estimators, voting=:soft)\nvc(X, y)  # Fit the classifier\npredictions = vc(X_test)  # Make predictions\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.VotingClassifier-Tuple{AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.VotingClassifier","text":"(vc::VotingClassifier)(X::AbstractMatrix, y::AbstractVector)\n\nFit the voting classifier.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values (class labels).\n\nReturns\n\nVotingClassifier: The fitted classifier.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.VotingClassifier-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.VotingClassifier","text":"(vc::VotingClassifier)(X::AbstractMatrix; type=nothing)\n\nPredict class labels for X.\n\nArguments\n\nX::AbstractMatrix: The input samples.\ntype: If set to :probs, return probability estimates for each class.\n\nReturns\n\nIf type is :probs, returns probabilities of each class.\nOtherwise, returns predicted class labels.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.ZeroEstimator","page":"API Reference","title":"NovaML.Ensemble.ZeroEstimator","text":"ZeroEstimator <: AbstractModel\n\nAn estimator that always predicts zero.\n\n\n\n\n\n","category":"type"},{"location":"api.html#NovaML.Ensemble.ZeroEstimator-Tuple{AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.ZeroEstimator","text":"(::ZeroEstimator)(X::AbstractMatrix)\n\nPredict using the zero estimator.\n\nArguments\n\nX::AbstractMatrix: The input samples.\n\nReturns\n\nVector{Float64}: Zero predictions.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, AdaBoostClassifier}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, model::AdaBoostClassifier)\n\nCustom show method for AdaBoostClassifier.\n\nArguments\n\nio::IO: The I/O stream.\nmodel::AdaBoostClassifier: The AdaBoost model to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, BaggingClassifier}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, bc::BaggingClassifier)\n\nCustom show method for BaggingClassifier.\n\nArguments\n\nio::IO: The I/O stream.\nbc::BaggingClassifier: The Bagging classifier to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, GradientBoostingClassifier}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, gbm::GradientBoostingClassifier)\n\nCustom show method for GradientBoostingClassifier.\n\nArguments\n\nio::IO: The I/O stream.\ngbm::GradientBoostingClassifier: The gradient boosting model to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, RandomForestClassifier}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, forest::RandomForestClassifier)\n\nCustom show method for RandomForestClassifier.\n\nArguments\n\nio::IO: The I/O stream.\nforest::RandomForestClassifier: The random forest classifier to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, RandomForestRegressor}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, forest::RandomForestRegressor)\n\nCustom show method for RandomForestRegressor.\n\nArguments\n\nio::IO: The I/O stream.\nforest::RandomForestRegressor: The random forest regressor to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Base.show-Tuple{IO, VotingClassifier}","page":"API Reference","title":"Base.show","text":"Base.show(io::IO, vc::VotingClassifier)\n\nCustom show method for VotingClassifier.\n\nArguments\n\nio::IO: The I/O stream.\nvc::VotingClassifier: The voting classifier to display.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble._compute_feature_importances-Tuple{AdaBoostClassifier}","page":"API Reference","title":"NovaML.Ensemble._compute_feature_importances","text":"_compute_feature_importances(model::AdaBoostClassifier)\n\nCompute feature importances for the AdaBoost model.\n\nArguments\n\nmodel::AdaBoostClassifier: The fitted AdaBoost model.\n\nReturns\n\nUnion{Vector{Float64}, Nothing}: The feature importances if available, otherwise nothing.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble._compute_oob_score-Tuple{BaggingClassifier, AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble._compute_oob_score","text":"_compute_oob_score(bc::BaggingClassifier, X::AbstractMatrix, y::AbstractVector)\n\nCompute out-of-bag score for the Bagging classifier.\n\nArguments\n\nbc::BaggingClassifier: The Bagging classifier.\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble._generate_indices-Tuple{BaggingClassifier, Int64}","page":"API Reference","title":"NovaML.Ensemble._generate_indices","text":"_generate_indices(bc::BaggingClassifier, n_samples::Int)\n\nGenerate sample indices for individual base estimators.\n\nArguments\n\nbc::BaggingClassifier: The Bagging classifier.\nn_samples::Int: The number of samples in the dataset.\n\nReturns\n\nVector{Int}: The generated sample indices.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.bootstrap_sample-Tuple{RandomForestClassifier, AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.bootstrap_sample","text":"bootstrap_sample(forest::RandomForestClassifier, X::AbstractMatrix, y::AbstractVector)\n\nCreate a bootstrap sample of the dataset.\n\nArguments\n\nforest::RandomForestClassifier: The random forest classifier.\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values.\n\nReturns\n\nTuple{AbstractMatrix, AbstractVector}: The bootstrapped samples and targets.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.calculate_tree_feature_importance-Tuple{DecisionTreeClassifier, Vector{Int64}, Int64}","page":"API Reference","title":"NovaML.Ensemble.calculate_tree_feature_importance","text":"calculate_tree_feature_importance(tree::DecisionTreeClassifier, feature_indices::Vector{Int}, n_features::Int)\n\nCalculate the feature importance for a single decision tree.\n\nArguments\n\ntree::DecisionTreeClassifier: The decision tree.\nfeature_indices::Vector{Int}: The indices of the features used in this tree.\nn_features::Int: The total number of features.\n\nReturns\n\nVector{Float64}: The feature importances.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.compute_feature_importances-Tuple{GradientBoostingClassifier}","page":"API Reference","title":"NovaML.Ensemble.compute_feature_importances","text":"compute_feature_importances(gbm::GradientBoostingClassifier)\n\nCompute feature importances for the gradient boosting model.\n\nArguments\n\ngbm::GradientBoostingClassifier: The fitted gradient boosting model.\n\nReturns\n\nVector{Float64}: The feature importances.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.compute_loss-Tuple{AbstractVector, AbstractVector, String}","page":"API Reference","title":"NovaML.Ensemble.compute_loss","text":"compute_loss(y::AbstractVector, y_pred::AbstractVector, loss::String)\n\nCompute the loss for the given predictions.\n\nArguments\n\ny::AbstractVector: The true values.\ny_pred::AbstractVector: The predicted values.\nloss::String: The loss function name.\n\nReturns\n\nFloat64: The computed loss.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.compute_negative_gradient-Tuple{AbstractVector, AbstractVector, String}","page":"API Reference","title":"NovaML.Ensemble.compute_negative_gradient","text":"compute_negative_gradient(y::AbstractVector, y_pred::AbstractVector, loss::String)\n\nCompute negative gradient for the given loss function.\n\nArguments\n\ny::AbstractVector: The true values.\ny_pred::AbstractVector: The predicted values.\nloss::String: The loss function name.\n\nReturns\n\nAbstractVector: The negative gradient.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.compute_oob_score-Tuple{RandomForestRegressor, AbstractMatrix, AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.compute_oob_score","text":"compute_oob_score(forest::RandomForestRegressor, X::AbstractMatrix, y::AbstractVector)\n\nCompute out-of-bag (OOB) score for the random forest regressor.\n\nArguments\n\nforest::RandomForestRegressor: The random forest regressor.\nX::AbstractMatrix: The input samples.\ny::AbstractVector: The target values.\n\nReturns\n\nTuple{Float64, Vector{Float64}}: The OOB score and OOB predictions.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.decision_function-Tuple{AdaBoostClassifier, AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.decision_function","text":"decision_function(model::AdaBoostClassifier, X::AbstractMatrix)\n\nCompute the decision function of X.\n\nArguments\n\nmodel::AdaBoostClassifier: The fitted AdaBoost model.\nX::AbstractMatrix: The input samples.\n\nReturns\n\nMatrix{Float64}: The decision function of the input samples.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.fit_initial_estimator-Tuple{AbstractVector}","page":"API Reference","title":"NovaML.Ensemble.fit_initial_estimator","text":"fit_initial_estimator(y::AbstractVector)\n\nFit an initial estimator based on the mean of y.\n\nArguments\n\ny::AbstractVector: The target values.\n\nReturns\n\nInitialEstimator: The fitted initial estimator.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.get_max_features-Tuple{RandomForestClassifier, Int64}","page":"API Reference","title":"NovaML.Ensemble.get_max_features","text":"get_max_features(forest::RandomForestClassifier, n_features::Int)\n\nGet the number of features to consider when looking for the best split.\n\nArguments\n\nforest::RandomForestClassifier: The random forest classifier.\nn_features::Int: The total number of features.\n\nReturns\n\nInt: The number of features to consider.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.get_max_features-Tuple{RandomForestRegressor, Int64}","page":"API Reference","title":"NovaML.Ensemble.get_max_features","text":"getmaxfeatures(forest::RandomForestRegressor, n_features::Int) Get the number of features to consider when looking for the best split.\n\nArguments\n\nforest::RandomForestRegressor: The random forest regressor.\nn_features::Int: The total number of features.\n\nReturns\n\nInt: The number of features to consider.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.get_params-Tuple{AdaBoostClassifier}","page":"API Reference","title":"NovaML.Ensemble.get_params","text":"get_params(model::AdaBoostClassifier; deep=true)\n\nGet parameters for this estimator.\n\nArguments\n\nmodel::AdaBoostClassifier: The AdaBoost model.\ndeep::Bool: If true, will return the parameters for this estimator and contained subobjects that are estimators.\n\nReturns\n\nDict: Parameter names mapped to their values.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.set_params!-Tuple{AdaBoostClassifier}","page":"API Reference","title":"NovaML.Ensemble.set_params!","text":"set_params!(model::AdaBoostClassifier; kwargs...)\n\nSet the parameters of this estimator.\n\nArguments\n\nmodel::AdaBoostClassifier: The AdaBoost model.\nkwargs...: Estimator parameters.\n\nReturns\n\nAdaBoostClassifier: The estimator instance.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.staged_predict-Tuple{AdaBoostClassifier, AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.staged_predict","text":"staged_predict(model::AdaBoostClassifier, X::AbstractMatrix)\n\nReturn a generator of predictions for each boosting iteration.\n\nArguments\n\nmodel::AdaBoostClassifier: The fitted AdaBoost model.\nX::AbstractMatrix: The input samples.\n\nReturns\n\nChannel: A generator of predictions at each stage.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.staged_predict_proba-Tuple{AdaBoostClassifier, AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.staged_predict_proba","text":"staged_predict_proba(model::AdaBoostClassifier, X::AbstractMatrix)\n\nReturn a generator of predicted probabilities for each boosting iteration.\n\nArguments\n\nmodel::AdaBoostClassifier: The fitted AdaBoost model.\nX::AbstractMatrix: The input samples.\n\nReturns\n\nChannel: A generator of predicted probabilities at each stage.\n\n\n\n\n\n","category":"method"},{"location":"api.html#NovaML.Ensemble.transform-Tuple{VotingClassifier, AbstractMatrix}","page":"API Reference","title":"NovaML.Ensemble.transform","text":"transform(vc::VotingClassifier, X::AbstractMatrix)\n\nReturn class labels or probabilities for X for each estimator.\n\nArguments\n\nvc::VotingClassifier: The fitted voting classifier.\nX::AbstractMatrix: The input samples.\n\nReturns\n\nIf voting is :soft, returns the probabilities for each class for each estimator.\nIf voting is :hard, returns the class label predictions of each estimator.\n\nThe shape of the return depends on the flatten_transform parameter.\n\n\n\n\n\n","category":"method"},{"location":"index.html#NovaML.jl","page":"Home","title":"NovaML.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"⚠️ IMPORTANT NOTE: NovaML.jl is currently in alpha stage. It is under active development and may contain bugs or incomplete features. Users should exercise caution and avoid using NovaML.jl in production environments at this time. We appreciate your interest and welcome feedback and contributions to help improve the package.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"NovaML.jl aims to provide a comprehensive and user-friendly machine learning framework written in Julia. Its objective is providing a unified API for various machine learning tasks, including supervised learning, unsupervised learning, and preprocessing, feature engineering etc.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Main objective of NovaML.jl is to increase the usage of Julia in daily data science and machine learning activities among students and practitioners.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Currently, the module and function naming in NovaML is similar to that of Scikit Learn to provide a familiarity to data science and machine learning practitioners. But NovaML is not a wrapper of ScikitLearn.","category":"page"},{"location":"index.html#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Unified API using Julia's multiple dispatch and functor-style callable objects\nAlgorithms for classification, regression, and clustering\nPreprocessing tools for data scaling, encoding, and imputation\nModel selection and evaluation utilities\nEnsemble methods","category":"page"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"You can install NovaML.jl using Julia's package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"pkg> add NovaML","category":"page"},{"location":"index.html#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The most prominent feature of NovaML is using functors (callable objects) to keep parameters as well as training and prediction. Assume model represents a supervised algorithm. The struct model keeps learned parameters and hyperparameters. It also behave as a function. ","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"model(X, y) trains the model. \nmodel(Xnew) calculates the predictions for Xnew. ","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Here's a quick example of how to use NovaML.jl for a binary classification task:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"using NovaML.Datasets\nX, y = load_iris(return_X_y=true)\n\nusing NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.2)\n\nusing NovaML.ModelSelection\nXtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.2)\n\n# Scale features\nusing NovaML.PreProcessing\nscaler = StandardScaler()\nscaler.fitted # false\n\n# Fit and transform\nXtrnstd = scaler(Xtrn) \n# transform with the fitted model\nXtststd = scaler(Xtst)\n\n# Train a model\nusing NovaML.LinearModel\nlr = LogisticRegression(η=0.1, num_iter=100)\n\nusing NovaML.MultiClass\novr = OneVsRestClassifier(lr)\n\n# Fit the model\novr(Xtrnstd, ytrn)\n\n# Make predictions\nŷtrn = ovr(Xtrnstd)\nŷtst = ovr(Xtststd)\n\n# Evaluate the model\nusing NovaML.Metrics\nacc_trn = accuracy_score(ytrn, ŷtrn);\nacc_tst = accuracy_score(ytst, ŷtst);\n\nprintln(\"Training accuracy: $acc_trn\")\nprintln(\"Test accuracy: $acc_tst\")\n# Training accuracy: 0.9833333333333333\n# Test accuracy: 0.9666666666666667","category":"page"}]
}
